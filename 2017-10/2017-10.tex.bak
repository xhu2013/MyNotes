% windsor's engineering notepad template
% uses emerald and lxfonts packages from ctan
%
% don't forget to change the date and other info in the header!!
%
% convert to bitmap with:
% convert -density 300x300 notes_template.pdf -resize 800x notes_template.png

\documentclass[12pt]{article}
\usepackage{CJK}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{emerald}
\usepackage{lxfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{eso-pic}
\usepackage{lastpage}
\usepackage[left=2.7cm,right=1.7cm,top=1.65cm,bottom=1.0cm]{geometry}





%插入带方框的文字
\usepackage{mdframed}
\usepackage{enumerate}
\usepackage{multirow}

%插入eps
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{subfigure}

%插入网址
\usepackage{url}

%插入超连接
\usepackage[colorlinks,linkcolor=blue]{hyperref}

%公式换行
\usepackage{amssymb,amsmath}


%边框

%字体颜色
\usepackage{color}





% configure page header
\usepackage{fancyhdr}
\setlength{\headheight}{15pt}
\pagestyle{fancy}\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\lhead{\normalfont\ECFAugie \large{MATH 3E NOTES}}
\chead{\normalfont\ECFAugie \large{W. SCHMIDT}\hspace{1.4cm}}
\rhead{\normalfont\ECFAugie \large{\MakeUppercase{1/27/11}}\hspace{1.5cm}\thepage\hspace{0.1cm}/\hspace{0.1cm}\pageref{LastPage}\hspace{-1.5cm}}

% background image
\newcommand\BackgroundPic{
\put(0,0){
\parbox[b][\paperheight]{\paperwidth}{%
\vfill
\centering
\includegraphics[width=\paperwidth,height=\paperheight,
keepaspectratio]{background.png}%
\vfill
}}}

% configure section titles
\usepackage{titlesec}
\titleformat{\section}{\Large}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}

\begin{document}
\begin{CJK*}{GBK}{kai}
\AddToShipoutPicture{\BackgroundPic}
\normalfont\ECFAugie

% custom commands
\newcommand{\windef}[1]{\subsection*{\underline{DEF:} \normalsize{#1}}} % definition
\newcommand{\winex}[1]{\subsection*{\underline{EX:} \normalsize{#1}}} % example
\newcommand{\winsec}[1]{\section*{\underline{#1}}} % section
\newcommand{\winres}[1]{\begin{math}\Rightarrow\left\{\begin{matrix}#1\end{matrix}\right.\end{math}\hspace{0.2cm}} % result
\newcommand{\winsys}[1]{\begin{math}\left\{\begin{matrix}#1\end{matrix}\right.\end{math}} % system
\newcommand{\winero}[1]{\begin{math}\xrightarrow{#1}\end{math}} % row operation
\newcommand{\winmat}[1]{\begin{math}\begin{bmatrix}#1\end{bmatrix}\end{math}} % matrix
\newcommand{\winsub}[1]{\subsection*{$\star$\hspace{0.2cm} #1}} % dot
\newcommand{\step}[1]{\begin{math}\xrightarrow{\text{#1}}\end{math}} % step in a process
\newcommand{\winrtwo}{\begin{math}\text{R}^\text{2}\end{math}}
\newcommand{\winrthree}{\begin{math}\text{R}^\text{3}\end{math}}
\newcommand{\wineq}[1]{\begin{equation}\notag#1\end{equation}}

%本人加的
\newcommand{\winsubsub}[1]{\subsubsection*{$\quad \diamond$\hspace{0.2cm} #1}} % dot
%%%%%%%%%%%%%%%%%%%%%%
% start writing here %
%%%%%%%%%%%%%%%%%%%%%%

\winsec{Optimal control}

Wikipedia: \href{https://en.wikipedia.org/wiki/Optimal_control}{Optimal control}

\winsub{General method}

Minimize the continuous-time cost functional
$$J=\Phi[x(t_0),t_0,x(t_f),t_f]+\int_{t_0}^{t_f}{{\cal L}[x(t),u(t),t]}dt$$
subject to the first-order dynamic constraints (the state equation)
$$\dot{x}(t)=a[x(t),u(t),t]$$
the algebraic path constraints
$$b[x(t),u(t),t]\leq 0$$
and the boundary conditions
$$\Phi[x(t_0),t_0,x(t_f),t_f]=0$$
where $x(t)$ is the state, $u(t)$ is the control, $t$ is the independent variable (generally speaking, time), $t_0$ is the initial time, and $t_f$ is the terminal time. The term $\Phi$ and ${\cal L}$ are called the endpoint cost and Lagrangian, respectively.

\winsub{Linear quadratic control}

A special case of the general nonlinear optimal control problem given in the previous section is the linear quadratic (LQ) optimal control problem. The LQ problem is stated as follows. Minimize the quadratic continuous-time cost functional
$$J=\frac{1}{2}x^T(t_f)S_fx(t_f)+\frac{1}{2}\int_{t_0}^{t_f}{[x^T(t)Q(t)x(t)+u^T(t)R(t)u(t)]dt}$$
Subject to the linear first-order dynamic constraints
$$\dot(x)=A(t)x(t)+B(t)u(t)$$
and the initial condition
$$x(t_0)=x_0$$
A particular form of the LQ problem that arises in many control system problems is that of the linear quadratic regulator (LQR) where all of the matrices (i.e., $A$, $B$, $Q$, and $R$) are constant, the initial time is arbitrarily set to zero, and the terminal time is taken in the limit $t_f\leftarrow \infty$ (this last assumption is what is known as infinite horizon). The LQR problem is stated as follows. Minimize the infinite horizon quadratic continuous-time cost functional
$$J=\frac{1}{2}\int_{0}^{\infty}{[x^T(t)Qx(t)+u^T(t)Ru(t)]dt}$$
Subject to the linear time-invariant first-order dynamic constraints
$$\dot{x}(t)=Ax(t)+Bu(t)$$
and the initial condition
$$x(t_0)=x_0$$
In the finite-horizon case the matrices are restricted in that $Q$ and $R$ are positive semi-definite and positive definite, respectively. In the infinite-horizon case, however, the matrices $Q$ and $R$ re not only positive-semidefinite and positive-definite, respectively, but are also constant. These additional restrictions on $Q$ and $R$ in the infinite-horizon case are enforced to ensure that the cost functional remains positive. Furthermore, in order to ensure that the cost function is bounded, the additional restriction is imposed that the pair $(A,B)$ is controllable. Note that the LQ or LQR cost functional can be thought of physically as attempting to minimize the control energy (measured as a quadratic form).

The infinite horizon problem (i.e., LQR) may seem overly restrictive and essentially useless because it assumes that the operator is driving the system to zero-state and hence driving the output of the system to zero. This is indeed correct. However the problem of driving the output to a desired nonzero level can be solved after the zero output one is. In fact, it can be proved that this secondary LQR problem can be solved in a very straightforward manner. It has been shown in classical optimal control theory that the LQ (or LQR) optimal control has the feedback form
$$u(t)=-K(t)x(t)$$
where $K(t)$ is a properly dimensioned matrix, given as
$$K(t)=R^{-1}B^TS(t)$$
and $S(t)$ is the solution of the differential Riccati equation. The differential Riccati equation is given as
$$\dot{S}(t)=-s(t)A-A^TS(t)+S(t)BR^{-1}B^TS(t)-Q$$
For the finite horizon LQ problem, the Riccati equation is integrated backward in time using the terminal boundary condition
$$S(t_f)=S_f$$
For the infinite horizon LQR problem, the differential Riccati equation is replaced with the algebraic Riccati equation (ARE) given as
$$0=-SA-A^TS+SBR^{-1}B^TS-Q$$
Understanding that the ARE arises from infinite horizon problem, the matrices $A$, $B$, $Q$ and $R$ are all constant. It is noted that there are in general multiple solutions to the algebraic Riccati equation and the positive definite (or positive semi-definite) solution is the one that is used to compute the feedback gain. The LQ (LQR) problem was elegantly solved by Rudolf Kalman.

\winsub{Numerical methods for optimal control}

\winsub{Discrete-time optimal control}

\winex{Finite time}
Consider the problem of a mine owner who must decide at what rate to extract ore from his mine. He owns rights to the ore from date $0$ and $T$. At date $0$ there is $x_0$ ore in the ground, and the instantaneous stock of ore $x(t)$ declines at the rate the mine owner extracts it $u(t)$. The mine owner extracts ore at cost $u(t)^2/x(t)$ and sells ore at a constant price $p$. He does not value the ore remaining in the ground at time $T$ (there is no "scrap value"). He chooses the rate of extraction in time $u(t)$ to maximize profits over the period of ownership with no time discounting.

\begin{figure}[!htb]
\begin{mdframed}
1. Discrete-time version

The manager maximizes profit $\Pi$:
$$\Pi=\sum_{t=0}^{T-1}{[pu_t-\frac{u_t^2}{x_t}]}$$
subject to the law of evolution for the state variable $x_t$
$$x_{t+1}-x_t=-u_t$$

Form the Hamiltonian and differentiate:
$$H=pu_t-\frac{u_t^2}{x_t}-\lambda_{t+1}u_t$$
$$\frac{\partial H}{\partial u_t}=p-\lambda_{t+1}-2\frac{u_t}{x_t}=0$$
$$\lambda_{t+1}-\lambda_t=-\frac{\partial H}{\partial x_t}=-(\frac{u_t}{x_t})^2$$

As the mine owner does not value the ore remaining at time $T$,
$$\lambda_T=0$$

Using the above equations, it is easy to solve for the $x_t$ and $\lambda_t$ series
$$\lambda_t=\lambda_{t+1}+\frac{(p-\lambda_{t+1})^2}{4}$$
$$x_{t+1}=x_t\frac{2-p+\lambda_{t+1}}{2}$$
and using the initial and turn-T conditions, the $x_t$ series can be solved explicitly, giving $u_t$.
\end{mdframed}
\caption{Discrete-time version}
\end{figure}

\begin{figure}[!htb]
\begin{mdframed}
2. Continuous-time version

The manager maximizes profit $\Pi$:
$$\Pi=\int_{0}^{T}{[pu(t)-\frac{u(t)^2}{x(t)}]dt}$$
subject to the law of evolution for the state variable $x(t)$
$$\dot{x}(t)=-u(t)$$

Form the Hamiltonian and differentiate:
$$H=pu(t)-\frac{u(t)^2}{x(t)}-\lambda(t)u(t)$$
$$\frac{\partial H}{\partial u}=p-\lambda(t)-2\frac{u(t)}{x(t)}=0$$
$$\dot{\lambda}(t)=-\frac{\partial H}{\partial x}=-(\frac{u(t)}{x(t)})^2$$

As the mine owner does not value the ore remaining at time $T$,
$$\lambda(T)=0$$

Using the above equations, it is easy to solve for the differential equations governing $u(t)$ and $\lambda(t)$
$$\dot{\lambda}(t)=-\frac{(p-\lambda(t))^2}{4}$$
$$u(t)=x(t)\frac{p-\lambda(t)}{2}$$
and using the initial and turn-T conditions, the functions can be solved to yield
$$x(t)=\frac{(4-pt+pT)^2}{(4+pT)^2}x_0$$
\end{mdframed}
\caption{Continuous-time version}
\end{figure}

\winsec{最优控制问题的理论和算法}

北京大学：\href{http://dsec.pku.edu.cn/~rli/research/doc/html/ocp.html}{最优控制问题的理论和算法}

\winsec{Optimal control}

Scholarpedia: \href{http://www.scholarpedia.org/article/Optimal_control}{Optimal control}

\winsub{Origins and applications}

Optimal control is closely related in its origins to the theory of calculus of variations.

\winsub{Formulation of optimal control problems}

The formulation of an optimal control problem requires the following:
\begin{itemize}
  \item a mathematical model of the system to be controlled,
  \item a specification of the performance index,
  \item a specification of all boundary conditions on states, and constraints to be satisfied by states and controls,
  \item a statement of what variables are free.
\end{itemize}

\winsub{Continuous time optimal control using the variational approach}

\winsubsub{General case with fixed final time and no terminal or path constraints}

Problem 1: Find the control vector trajectory ${\bf u}: [t_0,t_f]\in {\mathbb R}\mapsto {\mathbb R}^{n_u}$ to minimize the performance index:
\begin{equation}
J=\varphi({\bf x}(t_f))+\int_{t_0}^{t_f}{L({\bf x}(t),{\bf u}(t),t)dt}
\end{equation}
subject to:
\begin{equation}
\dot{\bf x}(t)={\bf f}({\bf x}(t),{\bf u}(t),t), \ {\bf x}(t_0)={\bf x}_0
\label{eq2}
\end{equation}
where $[t_0,t_f]$ is the time interval of interest, ${\bf x}: [t_0,t_f]\mapsto {\mathbb R}^{n_x}$ is the state vector; $\varphi: {\mathbb R}^{n_x}\times{\mathbb R}\mapsto{\mathbb R}$ is a terminal cost function, $L: {\mathbb R}^{n_x}\times{\mathbb R}^{n_u}\times{\mathbb R}\mapsto {\mathbb R}$ is an intermediate cost function, and ${\bf f}: {\mathbb R}^{n_x}\times{\mathbb R}^{n_u}\times{\mathbb R}\mapsto {\mathbb R}^{n_x}$ is vector field. Note that equation \ref{eq2} represents the dynamics of system and its initial state condition. Problem 1 as defined above is known as the Bolza problem. If $L({\bf x},{\bf u},t)=0$, then the problem is known as the Mayer problem, if $\varphi({\bf x}(t_f))=0$, it is known as the Lagrange problem. Note that the performance index $J=J({\bf u})$ is a function, this is a rule of correspondence that assigns a real value to each function ${\bf u}$ in a class. \href{http://www.scholarpedia.org/article/Optimal_control}{Calculus of variations} is concerned with the optimisation of functionals, and it is the tool that is used in this section to derive necessary optimality conditions for the minimisation of $J({\bf u})$.

Adjoin the constraints to the performance index with a time-varying Lagrange multiplier vector function $\lambda: [t_0,t_f]\mapsto {\mathbb R}^{n_x}$ (also known as the co-state), to define an augmented performance index $\bar{J}$:
\begin{equation}
\bar{J}=\varphi({\bf x}(t_f))+\int_{t_0}^{t_f}{\{L({\bf x},{\bf u},t)+\lambda^T(t)[{\bf f}({\bf x},{\bf u},t)-\dot{\bf x}]\}dt}
\end{equation}

Define the Hamiltonian function $H$ as follows:
\begin{equation}
H({\bf x}(t),{\bf u}(t),\lambda(t),t)=L({\bf x}(t),{\bf u}(t),t)+\lambda(t)^T{\bf f}({\bf x}(t),{\bf }u(t),t),
\end{equation}
such that $\bar{J}$ can be written as:
\begin{equation*}
\bar{J}=\varphi({\bf x}(t_f))+\int_{t_0}^{t_f}{\{H({\bf x}(t),{\bf u}(t),\lambda(t),t)-\lambda^T(t)\dot({\bf x})\}dt}
\end{equation*}

Assume that $t_0$ and $t_f$ are fixed. Now consider an infinitesimal variation in ${\bf u}(t)$, that is denoted as $\delta{\bf u}(t)$. Such a variation will produce variations in the state history $\delta{\bf x}(t)$, and a variation in the performance index $\delta\bar{J}$:
\begin{equation*}
\delta\bar{J}=[(\frac{\partial \varphi}{\partial{\bf x}}-\lambda^T)\delta{\bf x}]_{t=t_f}+[\lambda^T\delta{\bf x}]_{t=t_0}+\int_{t_0}^{t_f}{\{(\frac{\partial H}{\partial{\bf x}+\dot{\lambda}^T})\delta{\bf x}+(\frac{\partial H}{\partial{\bf u}})\delta{\bf u}\}dt}
\end{equation*}

Since the Lagrange multipliers are arbitrary, they can be selected to make the coefficients of $\delta{\bf x}(t)$ and $\delta{\bf x}(t_f)$ equal to zero, as follows:
\begin{equation}
\dot{\lambda}(t)^T=-\frac{\partial H}{\partial {\bf x}}
\end{equation}

\begin{equation}
\lambda(t_f)^T=\frac{\partial \varphi}{\partial {\bf x}}|_{t=t_f}
\end{equation}

This choice of $\lambda(t)$ results in the following expression for $\bar{J}$, assuming that the initial state is fixed, so that $\delta{\bf x}(t_0)$:
\begin{equation*}
\delta\bar{J}=\int_{t_0}^{t_f}{\{(\frac{\partial H}{\partial{\bf u}})\delta{\bf u}\}dt}
\end{equation*}

For a minimum, it is necessary that $\delta\bar{J}=0$. This gives the stationarity condition:
\begin{equation}
\frac{\partial H^T}{\partial {\bf u}}=0
\end{equation}

\winsubsub{The linear quadratic regulator}

A special case of optimal control problem which is of particular importance arises when the objective function is a quadratic function of ${\bf x}$ and ${\bf u}$, and the dynamic equations are linear. The resulting feedback law in the case is known as the linear quadratic regulator (LQR). The performance index is given by:
\begin{equation}
J=\frac{1}{2}{\bf x}(t_f)^T{\bf S}_f{\bf x}(t_f)+\frac{1}{2}\int_{t_0}^{t_f}{({\bf x}(t)^T{\bf Q}{\bf x}(t)+{\bf u}(t)^T{\bf R}{\bf u}(t))dt}
\end{equation}
where ${\bf S}_f$ and ${\bf Q}$ are positive semidefinite matrices, and ${\bf R}$ is a positive definite matrix, while the system dynamics obey:
\begin{equation}
\dot{\bf x}(t)={\bf A}{\bf x}(t)+{\bf B}{\bf u}(t),\ {\bf x}(t_0)={\bf x}_0
\end{equation}



















%eq2


\end{CJK*}
\end{document}
