% windsor's engineering notepad template
% uses emerald and lxfonts packages from ctan
%
% don't forget to change the date and other info in the header!!
%
% convert to bitmap with:
% convert -density 300x300 notes_template.pdf -resize 800x notes_template.png

\documentclass[12pt]{article}
\usepackage{CJK}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage{emerald}
\usepackage{lxfonts}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{eso-pic}
\usepackage{lastpage}
\usepackage[left=2.7cm,right=1.7cm,top=1.65cm,bottom=1.0cm]{geometry}





%插入带方框的文字
\usepackage{mdframed}
\usepackage{enumerate}
\usepackage{multirow}

%插入eps
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{subfigure}

%插入网址
\usepackage{url}

%插入超连接
\usepackage[colorlinks,linkcolor=blue]{hyperref}




\usepackage{pythonhighlight}






% configure page header
\usepackage{fancyhdr}
\setlength{\headheight}{15pt}
\pagestyle{fancy}\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\lhead{\normalfont\ECFAugie \large{MATH 3E NOTES}}
\chead{\normalfont\ECFAugie \large{W. SCHMIDT}\hspace{1.4cm}}
\rhead{\normalfont\ECFAugie \large{\MakeUppercase{1/27/11}}\hspace{1.5cm}\thepage\hspace{0.1cm}/\hspace{0.1cm}\pageref{LastPage}\hspace{-1.5cm}}

% background image
\newcommand\BackgroundPic{
\put(0,0){
\parbox[b][\paperheight]{\paperwidth}{%
\vfill
\centering
\includegraphics[width=\paperwidth,height=\paperheight,
keepaspectratio]{background.png}%
\vfill
}}}

% configure section titles
\usepackage{titlesec}
\titleformat{\section}{\Large}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}

\begin{document}
\begin{CJK*}{GBK}{kai}
\AddToShipoutPicture{\BackgroundPic}
\normalfont\ECFAugie

% custom commands
\newcommand{\windef}[1]{\subsection*{\underline{DEF:} \normalsize{#1}}} % definition
\newcommand{\winex}[1]{\subsection*{\underline{EX:} \normalsize{#1}}} % example
\newcommand{\winsec}[1]{\section*{\underline{#1}}} % section
\newcommand{\winres}[1]{\begin{math}\Rightarrow\left\{\begin{matrix}#1\end{matrix}\right.\end{math}\hspace{0.2cm}} % result
\newcommand{\winsys}[1]{\begin{math}\left\{\begin{matrix}#1\end{matrix}\right.\end{math}} % system
\newcommand{\winero}[1]{\begin{math}\xrightarrow{#1}\end{math}} % row operation
\newcommand{\winmat}[1]{\begin{math}\begin{bmatrix}#1\end{bmatrix}\end{math}} % matrix
\newcommand{\winsub}[1]{\subsection*{$\star$\hspace{0.2cm} #1}} % dot
\newcommand{\step}[1]{\begin{math}\xrightarrow{\text{#1}}\end{math}} % step in a process
\newcommand{\winrtwo}{\begin{math}\text{R}^\text{2}\end{math}}
\newcommand{\winrthree}{\begin{math}\text{R}^\text{3}\end{math}}
\newcommand{\wineq}[1]{\begin{equation}\notag#1\end{equation}}

%%%%%%%%%%%%%%%%%%%%%%
% start writing here %
%%%%%%%%%%%%%%%%%%%%%%






\winsec{获得授权翻译斯坦福CS231n课程笔记系列}

\href{https://zhuanlan.zhihu.com/p/20870307}{获得授权翻译斯坦福CS231n课程笔记系列}

\winsec{CS231n课程笔记翻译：Python Numpy教程}

\href{https://zhuanlan.zhihu.com/p/20878530}{CS231n 课程笔记翻译：Python Numpy教程}

\href{http://scipy.github.io/old-wiki/pages/NumPy_for_Matlab_Users}{numpy for Matlab users}


%\begin{python}
%def f(x):
%    return x
%\end{python}

\winsec{CS231n课程笔记翻译：图像分类笔记（上）}

\href{https://zhuanlan.zhihu.com/p/20894041}{CS231n 课程笔记翻译：图像分类笔记（上）}

\winsub{图像分类}

\windef{图像分类}所谓图像分类问题，就是已有固定的分类标签集合，然后对于输入的图像，从分类标签集合中找出一个分类标签，最后把分类标签分配给该输入图像。

{\bf 困难和挑战：}视角变化(Viewpoint variation)、大小变化(Scale variation)、形变(Deformation)、遮挡(Occlusion)、光照条件(Illumination conditions)、背景干扰(Background clutter)、类内差异(Intra-class variation)

\windef{数据驱动方法}给计算机很多数据，然后实现学习算法，让计算机学习到每个类的外形。这种方法，就是数据驱动方法。

数据驱动方法的第一步就是收集已经做好分类标注的图片来作为训练集。

{\bf 图像分类流程：}输入(输入训练集)
$\Longrightarrow$学习(训练分类哭或者学习模型)$\Longrightarrow$评价(评价分类器)

\winsub{Nearest Neighbor分类器}

{\bf 图像分类数据集：}\href{http://www.cs.toronto.edu/~kriz/cifar.html}{CIFAR-10}

\windef{Nearest Neighbor算法}Nearest Neightbor算法将会拿差测试图片和训练集中每一张图片比较，然后将它认为最相似的那个训练集图片的标签赋给这张测试图片。

\begin{itemize}
\item {\bf L1距离} $$d_1(I_1,I_2)=\sum_p{|I_1^p-I_2^p|}$$
\item {\bf L2距离} $$d_2(I_1,I_2)=\sqrt{\sum_p{(I_1^p-I_2^p)^2}}$$
\end{itemize}

\winex{Nearest Neighbor for CIFAR-10}
基于CIFAR-10数据的Kaggle算法竞赛排行榜：\href{https://www.kaggle.com/c/cifar-10/leaderboard}{CIFAR-10 - Object Recognition in Images}

\winsub{K-Nearest Neighbor分类器}

\windef{K-Nearest Neighbor分类器}其思想：与其只找最相近的那1个图片的标签，不如找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。

\winsec{CS231n课程笔记翻译：图像分类笔记（下）}

\href{https://zhuanlan.zhihu.com/p/20900216}{CS231n 课程笔记翻译：图像分类笔记（下）}

\winsub{用于超参数调优的验证集}

\windef{超参数(hyperparameter)}
k-NN分类器需要设定k值，那么选择哪个k值最合适的呢？对于不同的距离函数，比如L1范数和L2范数等，那么选哪个好？还有不少选择需要考虑到(比如：点积)。所有这些选择，被称为{\bf 超参数(hyperparameter)}。

{\bf Remark: }决不能使用测试集来进行调优。

测试数据集只使用一次，即在训练完成后评价最终的模型时使用。

当你在设计机器学习算法的时候，应该把测试集看做非常珍贵的资源，不到最后一步，绝不使用它。如果你使用测试集来调优，而且算法看起来效果不错，那么真正的危险在于：算法实际部署后，性能可能会远低于预期。这种情况，称之为算法对测试集{\bf 过拟合}。

从训练集中取出一部分数据用来调优，我们称之为{\bf 验证集（validation set）}。验证集其实就是作为假的测试集来调优。

把训练集分成训练集和验证集。使用验证集来对所有超参数调优。最后只在测试集上跑一次并报告结果。


当训练集数量较小时（因此验证集的数量更小），使用{\bf 交叉验证}的方法。

\winsub{Nearest Neighbor分类器的优劣}

实现简单，训练时间短，测试时间长。但在实际应用中，更关注测试效率。

\winsec{CS231n课程笔记翻译：线性分类笔记（上）}

\href{https://zhuanlan.zhihu.com/p/20918580}{CS231n 课程笔记翻译：线性分类笔记（上）}

\winsub{线性分类}

\begin{enumerate}[]
  \item {\bf 评分函数(score function)：}原始图像数据到类别分值的映射。
  \item {\bf 损失函数(loss function)：}用来量化预测分类标签的得分与真实标签之间一致性的。
\end{enumerate}

\winsub{从图像到标签分值的参数化映射}

\begin{enumerate}[]
  \item {\bf 线性分类器}$$f(x_i,W,b)=Wx_i+b$$
\end{enumerate}



\winsub{理解线性分类器}

\begin{enumerate}[]
  \item {\bf 将图像看做高维度的点}
  \item {\bf 将线性分类器看做模板匹配}
  \item {\bf 偏差和权重的合并技巧}$$f(x_i,W,b)=Wx_i+b\Longrightarrow f(x_i,W)=Wx_i$$
  \item {\bf 图像数据预处理}：零均值的中心化
\end{enumerate}

\winsec{CS231n课程笔记翻译：线性分类笔记（中）}

\href{https://zhuanlan.zhihu.com/p/20945670}{CS231n 课程笔记翻译：线性分类笔记（中）}

\winsub{损失函数 Loss function}

\windef{损失函数(Loss Function)}使用{\bf 损失函数(L oss Function)}(有时也叫{\bf 代价函数Cost Function}或{\bf 目标函数Objective})来衡量我们对结果的不满意程度。直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。

\winsub{多类支持向量机损失 Multiclass Support Vector Machine Loss}

\windef{多类支持向量机（SVM）损失函数}SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值$\Delta$。

针对第$i$个数据的多类SVM的损失函数定义如下：$$L_i=\sum_{j\neq y_i}{\max{(0,s_j-s_{y_i}+\Delta)}}$$

简而言之，SVM的损失函数想要正确分类类别$y_i$的分数比不正确类别分数高，而且至少要高$\Delta$。如果不满足这点，就开始计算损失值。

对于线性评分函数($f(x_i,W)=Wx_i$)，将损失函数的公式改写如下：
$$L_i=\sum_{j\neq y_i}{\max{(0,w_j^Tx_i-w_{y_i}^Tx_i+\Delta)}}$$其中$w_j$ 是权重$W$的第$j$行，被变形为列向量。

$\max{(0,-)}$函数，它常被称为{\bf 折叶损失(hinge loss)}。 有时候会听到人们使用平方折叶损失SVM(即L2-SVM)，它使用的是$\max{(0,-)^2}$，将更强烈(平方地而不是线性地)地惩罚过界的边界值。

我们对于预测训练集数据分类标签的情况总有一些不满意的，而损失函数就能将这些不满意的程度量化。

{\bf 正则化惩罚(regularization penalty)}$$R(W)=\sum_k{\sum_l{W_{k,l}^2}}$$

包含正则化惩罚后，就能够给出完整的多类SVM损失函数了，它由两个部分组成：{\bf 数据损失(data loss)}，即所有样例的的平均损失$L_i$，以及{\bf 正则化损失(regularization loss)}。完整公式如下所示：$$L=\underbrace{\frac{1}{N}\sum_i{L-i}}_{data\ loss}+\underbrace{\lambda R(W)}_{regularization\ loss}$$将其展开完整公式是：$$L=\frac{1}{N}\sum_i{\sum_{j\neq y_i}{[\max{(0,f(x_i;W)_j-f(x_i;W)_{y_i}+\Delta)}]}}+\lambda\sum_k{\sum_l{W_{k,l}^2}}$$ 其中，$N$是训练集的数据量。现在正则化惩罚添加到了损失函数里面，并用超参数$\lambda$来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。

\winsub{实际考虑}

\begin{enumerate}[]
  \item {\bf 设置$\Delta$}
  \item {\bf 与二元支持向量机(Binary Support Vector Machine)的关系：}二元支持向量机对第$i$个数据的损失计算公式是：$$L_i=C\max{(0,1-y_iw^Tx_i)}+R(W)$$其中，$C$ 是一个超参数，并且$y_i\in \{-1,1\}$。公式中的$C$和多类SVM公式中的$\lambda$都控制着同样的权衡，而且它们之间的关系是$C\propto \frac{1}{\lambda}$。
  \item {\bf 备注：在初始形式中进行最优化}
  \item {\bf 备注：其他多类SVM公式}
\end{enumerate}

\winsub{CS231n课程笔记翻译：线性分类笔记（下）}

\href{https://zhuanlan.zhihu.com/p/21102293}{CS231n 课程笔记翻译：线性分类笔记（下）}

\winsub{Softmax分类器}
{\bf 交叉熵损失(cross-entropy loss)}$$L_i=-\log{(\frac{e^{f_{y_i}}}{\sum_j{e^{f_j}}})}$$
或等价的$$L_i=-f_{y_i}+\log{(\sum_j{e^{f_j}})}$$

函数$f_j(z)=\frac{c^{z_j}}{\sum_k{e^{z_k}}}$被称作{\bf softmax函数}：其输入值是一个向量，向量中元素为任意实数的评分值($z$中的)，函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。

{\bf 信息理论视角：}在“真实”分布$p$和估计分布$q$之间的交叉熵定义如下：$$H(p,q)=-\sum_x{p(x)\log{q(x)}}$$

Kullback-Leibler差异（Kullback-Leibler Divergence）也叫做相对熵（Relative Entropy），它衡量的是相同事件空间里的两个概率分布的差异情况。

{\bf 概率论解释：}$$P(y_i|x_i,W)=\frac{e^{f_{y_i}}}{\sum_j{e^{f_j}}}$$ 可以解释为是给定图像数据$x_i$，以$W$为参数，分配给正确分类标签$y_i$的归一化概率。

\winsub{SVM和Softmax的比较}

\winex{SVM和Softmax的比较}
\begin{figure}[!htb]
\centering
\includegraphics[width=0.75\columnwidth]{svmvssoftmax.png}
\caption{SVM和Softmax的比较}
\label{svmvssoftmax}
\end{figure}

两个分类器都计算了同样的分值向量$f$(本节中是通过矩阵乘来实现)。不同之处在于对$f$中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类(本例中是蓝色的类别2)的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的{\bf 对数概率}，鼓励正确分类的归一化的对数概率变高，其余的变低。

\begin{enumerate}[]
  \item {\bf Softmax分类器为每个分类提供了“可能性”}
  \item {\bf 在实际使用中，SVM和Softmax经常是相似的}
\end{enumerate}

\winsec{CS231n课程笔记翻译：最优化笔记（上）}

\href{https://zhuanlan.zhihu.com/p/21360434}{CS231n 课程笔记翻译：最优化笔记（上）}

\winsub{简介}

图像分类任务中的两个关键部分：
\begin{enumerate}
  \item 基于参数的{\bf 评分函数}。该函数将原始图像像素映射为分类评分值(例如：一个线性函数)。
  \item {\bf 损失函数}。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式(例如：Softmax或SVM)。
\end{enumerate}

线性函数的形式是$f(x_i,W)=Wx_i$，而SVM实现的公式是：$$L=\frac{1}{N}\sum_i{\sum_{j\neq y_i}{[\max{(0,f(x_i;W)_j-f(x_i;W)_{y_i}+1)}]+\alpha R(W)}}$$

\windef{最优化Optimization}最优化是寻找能使得损失函数值最小化的参数$W$的过程。

\winsub{损失函数可视化}

\begin{enumerate}[]
  \item \href{http://stanford.edu/~boyd/cvxbook/}{凸函数最优化}
  \item \href{https://en.wikipedia.org/wiki/Subderivative}{次梯度(subgradient)}
\end{enumerate}

\winsub{最优化 Optimization}

损失函数可以量化某个具体权重集$W$的质量。而最优化的目标就是找到能够最小化损失函数值的$W$。

\noindent {\bf 策略\#1：一个差劲的初始方案：随机搜索}

{\bf 核心思路：迭代优化}
从随机权重开始，然后迭代取优，从而获得更低的损失值。

\noindent {\bf 策略\#2：随机本地搜索}

从一个随机$W$开始，然后生成一个随机的扰动$\delta W$，只有当$W+\delta W$的损失值变低，我们才会更新。

\noindent {\bf 策略\#3：跟随梯度}

从数学上计算出最陡峭的方向，这个方向就是损失函数的{\bf  梯度(gradient)}。

对一维函数的求导公式如下：
$$\frac{df(x)}{dx}=\lim_{h\rightarrow 0}{\frac{f(x+h)-f(x)}{h}}$$

\winsec{CS231n课程笔记翻译：最优化笔记（下）}

\href{https://zhuanlan.zhihu.com/p/21387326}{CS231n 课程笔记翻译：最优化笔记（下）}

\winsub{梯度计算}

计算梯度有两种方法：一个是缓慢的近似方法({\bf 数值梯度法})，但实现相对简单。另一个方法({\bf 分析梯度法})计算迅速，结果精确，但是实现时容易出错，且需要使用微分。

\winsub{利用有限差值计算梯度}

\begin{enumerate}[]
  \item {\bf 实践考量：}中心差值公式(centered difference formula)$[f(x+h)-f(x-h)]/2h$
  \item {\bf 在梯度负方向上更新}
  \item {\bf 步长的影响}
  \item {\bf 效率问题}
\end{enumerate}

\winsub{微分分析计算梯度}

SVM的损失函数
$$L_i=\sum_{j\neq y_i}{[\max{(0,w_j^Tx_i-w_{j_i}^{T}x_i+\Delta)}]}$$
关于$w_{y_i}$对其进行微分得到：
$$\nabla_{w_{j_i}}L_i=-(\sum_{j\neq y_i}{\mathbb{I}(w_j^Tx_i-w_{y_i}^Tx_i+\Delta>0)})x_i$$

注意，这个梯度只是对应正确分类的$W$的行向量的梯度，那些$j\neq y_i$行的梯度是：$$\nabla_{w_j}L_i=\mathbb{I}(w_j^Tx_i-w_{y_i}{T}x_i+\Delta>0)x_i$$

\winsub{梯度下降}

\begin{enumerate}[]
  \item {\bf 小批量数据梯度下降(Mini-batch gradient descent)}
  \item {\bf 随机梯度下降(Stochastic Gradient Descent 简称SGD)}
\end{enumerate}

\winsec{斯坦福CS231n课程作业\# 1简介}

\href{https://zhuanlan.zhihu.com/p/21441838}{斯坦福CS231n课程作业\# 1简介}

\winsec{CS231n课程笔记翻译：反向传播笔记}

\href{https://zhuanlan.zhihu.com/p/21407711}{CS231n 课程笔记翻译：反向传播笔记}

\winsub{简介}

{\bf 问题陈述}：给定函数$f(x)$，其中$x$是输入数据的向量，需要计算函数$f$关于$x$的梯度，也就是$\nabla f(x)$。

\winsub{简单表达式和理解梯度}

函数关于每个变量的导数指明了整个表达式对于该变量的敏感程度。

\winsub{使用链式法则计算复合表达式}

链式法则指出将梯度表达式链接起来的正确方式是相乘，比如$\frac{\partial f}{\partial x}=\frac{\partial f}{\partial q}\frac{\partial q}{\partial x}$。

\winsub{反向传播的直观理解}

链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。

\winsub{模块化：Sigmoid例子}

sigmoid函数关于其输入的求导是可以简化的：
$$\sigma (x)=\frac{1}{1+e^{-x}}\rightarrow \frac{d \sigma (x)}{dx}=\frac{e^{-x}}{(1+e^{-2})^2}=(\frac{1+e^{-x}-1}{1+e^{-x}})(\frac{1}{1+e^{-x}})=(1-\sigma (x))\sigma (x)$$

{\bf 实现提示：分段反向传播}

\winsub{反向传播实践：分段计算}

\begin{enumerate}[]
  \item {\bf 对前向传播变量进行缓存}
  \item {\bf 在不同分支的梯度要相加}
\end{enumerate}

\winsub{回传流中的模式}

加法操作将梯度相等地分发给它的输入。取最大操作将梯度路由给更大的输入。乘法门拿取输入激活数据，对它们进行交换，然后乘以梯度。

\winsub{用向量化操作计算梯度}

{\bf 矩阵相乘的梯度：}可能最有技巧的操作是矩阵相乘（也适用于矩阵和向量，向量和向量相乘）的乘法操作。











% section
%\winsec{section name goes here}

% term definition
%\winsub{term definition}
%\windef{term - and it's definition}

% example
%\winsub{an example}
%\winex{example heading}

% system of equations
%\winsub{a system of equations}
%\winsys{
%2x+4y=2 \\
%2x+6y=3}
%
% matrix
%\winsub{working a multistep problem}
%\winmat{
%a & b & c \\
%d & e & f \\
%g & h & i }
% row operation
%\winero{R_1+R_2}
% matrix
%\winmat{
%a & b & c \\
%d & e & f \\
%g & h & i }
% result
%\winres{
%x = 1 \\
%y = 2 \\
%z = 3}

% vector in 3-space
%\winsub{a vector in \winrthree}
%v = \winmat{
%x \\
%y \\
%z }

%\winsub{a multi-step process}
%A \step{do stuff} B \step{more stuff} C

% numbered list
%\winsub{an enumerated list}
%\begin{enumerate}
%\item{this is the first item in an enumerated list}
%\item{this is the second item in an enumerated list}
%\end{enumerate}

% manual line breaks
%\winsub{manually broken lines}
%the first line\\
%the second line\\
%the third line

% equation
%\winsub{some math}
%\wineq{
%\int_a^b \! f(x) \; dx % definite
%\int \! f(x) \, dx \; % indefinite
%\frac{\pi}{2} \; % fraction
%\sqrt{\theta} \; % root
%n = 1,2,3 \ldots 4}




\end{CJK*}
\end{document}
