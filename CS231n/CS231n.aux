\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces SVM和Softmax的比较}}{5}{figure.1}}
\newlabel{svmvssoftmax}{{1}{5}{SVM和Softmax的比较\relax }{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces 左边是Sigmoid非线性函数，将实数压缩到$[0,1]$之间。右边是tanh函数，将实数压缩到$[-1,1]$。}}{9}{figure.2}}
\newlabel{sigmoidtanh}{{2}{9}{左边是Sigmoid非线性函数，将实数压缩到$[0,1]$之间。右边是tanh函数，将实数压缩到$[-1,1]$。\relax }{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces 左边是ReLU(校正线性单元：Rectified Linear Unit)激活函数，当$x=0$时函数值为0。当$x>0$函数的斜率为1。右边是从\href  {http://www.cs.toronto.edu/\nobreakspace  {}fritz/absps/imagenet.pdf}{Krizhevsky} 等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。}}{10}{figure.3}}
\newlabel{relu}{{3}{10}{左边是ReLU(校正线性单元：Rectified Linear Unit)激活函数，当$x=0$时函数值为0。当$x>0$函数的斜率为1。右边是从\href {http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf}{Krizhevsky} 等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。\relax }{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces 左边是一个2层神经网络，隐层由4个神经元(也可称为单元(unit))组成，输出层由2个神经元组成，输入层是3 个神经元。右边是一个3层神经网络，两个含4个神经元的隐层。注意：层与层之间的神经元是全连接的，但是层内的神经元不连接。}}{11}{figure.4}}
\newlabel{fullyconnectedlayer}{{4}{11}{左边是一个2层神经网络，隐层由4个神经元(也可称为单元(unit))组成，输出层由2个神经元组成，输入层是3 个神经元。右边是一个3层神经网络，两个含4个神经元的隐层。注意：层与层之间的神经元是全连接的，但是层内的神经元不连接。\relax }{figure.4}{}}
\newlabel{LastPage}{{}{13}{}{page.13}{}}
\xdef\lastpage@lastpage{13}
\xdef\lastpage@lastpageHy{13}
