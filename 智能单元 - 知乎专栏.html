<!DOCTYPE html>
<!-- saved from url=(0042)https://zhuanlan.zhihu.com/intelligentunit -->
<html lang="zh-CN" class="no-touch"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>智能单元 - 知乎专栏</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
    <link rel="shortcut icon" href="https://static.zhihu.com/static/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="./智能单元 - 知乎专栏_files/app.6717c2be0ab84ed836aeb0f64393380d.css">
    <style></style>
    <script>document.documentElement.className += ('ontouchstart' in window) ? ' touch' : ' no-touch'</script>
  </head>
  <body class="">

    <div id="react-root"><div data-reactroot=""><div class="Layout av-cardBackground" data-zop-usertoken="{&quot;userToken&quot;:&quot;xhu2013&quot;}"><div class="Layout-navbarHolder"><header class="Navbar"><div class="Navbar-logo-wrapper"><a class="Navbar-logo icon-ic_zhihu_logo" href="https://www.zhihu.com/" target="_blank" rel="noopener" aria-label="知乎首页"></a></div><div class="Navbar-columnTitle Navbar-title"><img class="Navbar-columnIcon" alt="智能单元" src="./智能单元 - 知乎专栏_files/4a97d93d652f45ededf2ebab9a13f22b_m.jpeg"><div class="Navbar-columnTitleMain">智能单元</div><button class="Button ColumnFollowButton Navbar-followBtn" type="button">已关注</button></div><div class="Navbar-functionality"><a class="Navbar-write"><i class="icon icon-ic_nav_new"></i><!-- react-text: 544 -->写文章<!-- /react-text --></a><div class="Menu Navbar-menu"><button class="Button Button Button--plain MenuButton MenuButton-listen-hover icon icon-ic_nav_more" aria-label="更多选项" type="button"></button><div class="Menu-dropdown" style="visibility: hidden;"></div></div></div></header></div><!-- react-empty: 548 --><div></div><div class="Layout-main av-card"><div class="ColumnAbout"><img src="./智能单元 - 知乎专栏_files/4a97d93d652f45ededf2ebab9a13f22b_xl.jpeg" class="ColumnAbout-avatar" alt=""><h1 class="ColumnAbout-name">智能单元</h1><p class="ColumnAbout-intro">聚焦通用人工智能</p><div><div class="ColumnAbout-actions"><button class="Button ColumnFollowButton" type="button">已关注</button><div class="Menu ColumnAbout-menu"><button class="Button Button Button--plain MenuButton MenuButton-listen-click ColumnAbout-menuButton icon icon-ic_unfold" aria-label="更多操作" type="button"></button><div class="Menu-dropdown" style="visibility: hidden;"></div></div></div><a class="ColumnAbout-followers" href="https://zhuanlan.zhihu.com/intelligentunit/followers" target="_blank"><!-- react-text: 562 -->31425<!-- /react-text --><!-- react-text: 563 --> 人关注<!-- /react-text --></a></div><ul class="ColumnTopicList"><li><a href="https://zhuanlan.zhihu.com/intelligentunit"><span class="TopicTag is-active"><!-- react-text: 568 -->全部<!-- /react-text --><span class="TopicTag-count">80</span></span></a></li><li><a href="https://zhuanlan.zhihu.com/intelligentunit?topic=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%88Deep%20Learning%EF%BC%89"><span class="TopicTag"><!-- react-text: 573 -->深度学习（Deep Learning）<!-- /react-text --><span class="TopicTag-count">56</span></span></a></li><li><a href="https://zhuanlan.zhihu.com/intelligentunit?topic=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD"><span class="TopicTag"><!-- react-text: 578 -->人工智能<!-- /react-text --><span class="TopicTag-count">51</span></span></a></li><li><a href="https://zhuanlan.zhihu.com/intelligentunit?topic=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="TopicTag"><!-- react-text: 583 -->机器学习<!-- /react-text --><span class="TopicTag-count">25</span></span></a></li><li><button class="ColumnTopicList-showall" type="button">更多</button></li></ul></div><div class="ColumnPins"><div class="BlockTitle av-marginLeft av-borderColor"><span class="BlockTitle-title">置顶文章</span><span class="BlockTitle-line"></span></div><div class="PostListItem PostListItem--column"><!-- react-text: 592 --><!-- /react-text --><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22339097"><span class="PostListItem-title">智能单元专栏目录</span><p class="PostListItem-summary"><!-- react-text: 597 -->智能单元 - 知乎专栏长期原创和翻译深度学习和深度增强学习等领域高质量文章，并接受知友投稿，投稿前请阅读投稿说明。最前沿系列最前沿系列解读我们认为的深度学习领域有巨大影响的论文和…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 599 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 9月 11 日星期日晚上 9 点 13 分"><time datetime="2016-09-11T21:13:02+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22339097"><!-- react-text: 609 -->150<!-- /react-text --><!-- react-text: 610 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22339097#comments"><!-- react-text: 613 -->16<!-- /react-text --><!-- react-text: 614 --> 条评论<!-- /react-text --></a></span></div></div></div></div><div><div class="BlockTitle av-marginLeft av-borderColor"><h2 class="BlockTitle-title">最新文章</h2><span class="BlockTitle-line"></span></div><div class="InfiniteList ColumnPostList"><ul><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/27699682" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-a56eac8ea11f19aa5196182cb403aedc_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/27699682"><span class="PostListItem-title">荐译一篇通俗易懂的策略梯度方法讲解</span><p class="PostListItem-summary"><!-- react-text: 629 -->版权声明：本文智能单元首发，本人独立原创翻译，禁止未授权转载。前言：策略梯度（Policy Gradient）类方法是增强学习的重要组成部分。关于策略梯度的讲解，有David Silve…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 631 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 7月 23 日星期日下午 4 点 17 分"><time datetime="2017-07-23T16:17:05+08:00">18 天前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/27699682"><!-- react-text: 641 -->132<!-- /react-text --><!-- react-text: 642 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/27699682#comments"><!-- react-text: 645 -->10<!-- /react-text --><!-- react-text: 646 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/27012520" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-516e60756ae97e58fd3022e548d52b03_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/27012520"><span class="PostListItem-title">从头开始GAN</span><p class="PostListItem-summary"><!-- react-text: 655 -->1 前言GAN的火爆想必大家都很清楚了，各种GAN像雨后春笋一样冒出来，大家也都可以名正言顺的说脏话了[微笑脸]。虽然目前GAN的酷炫应用还集中在图像生成上，但是GAN也已经拓…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 657 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 7月 18 日星期二中午 12 点 22 分"><time datetime="2017-07-18T12:22:19+08:00">23 天前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/27012520"><!-- react-text: 667 -->370<!-- /react-text --><!-- react-text: 668 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/27012520#comments"><!-- react-text: 671 -->21<!-- /react-text --><!-- react-text: 672 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/27935902" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-3ce09841f5ec27b18afa2f741512b78e_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/27935902"><span class="PostListItem-title">最前沿：机器人学习Robot Learning之模仿学习Imitation Learning的发展</span><p class="PostListItem-summary"><!-- react-text: 681 -->1 前言在上一篇文章最前沿：机器人学习Robot Learning的发展 - 知乎专栏 中，我们介绍了机器人学习Robot Learning这个方向的发展趋势，并介绍了部分基于DRL的方法，那么在…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 683 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 7月 18 日星期二中午 12 点 02 分"><time datetime="2017-07-18T12:02:49+08:00">23 天前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/27935902"><!-- react-text: 693 -->84<!-- /react-text --><!-- react-text: 694 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/27935902#comments"><!-- react-text: 697 -->14<!-- /react-text --><!-- react-text: 698 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/27860621" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-3ce09841f5ec27b18afa2f741512b78e_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/27860621"><span class="PostListItem-title">DRL实战：用PyTorch 150行代码实现Advantage Actor-Critic玩CartPole</span><p class="PostListItem-summary"><!-- react-text: 707 -->1 前言今天我们来用Pytorch实现一下用Advantage Actor-Critic 也就是A3C的非异步版本A2C玩CartPole。2 前提条件要理解今天的这个DRL实战，需要具备以下条件：理解Advantage…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 709 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 7月 13 日星期四下午 1 点 04 分"><time datetime="2017-07-13T13:04:06+08:00">1 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/27860621"><!-- react-text: 719 -->60<!-- /react-text --><!-- react-text: 720 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/27860621#comments"><!-- react-text: 723 -->23<!-- /react-text --><!-- react-text: 724 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/27696130" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-b8e70630d579926c5a13e0e19580e05c_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/27696130"><span class="PostListItem-title">机器人革命与学会学习Learning to Learn</span><p class="PostListItem-summary"><!-- react-text: 733 -->1 前言当前深度学习革命正在如火如荼的展开，视觉，语音，医疗，自动驾驶等等各种各样的初创公司遍地开花，人工智能进入了最好的时代。然而，对于人工智能的应用，最最重要…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 735 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 7月 5 日星期三下午 12 点 38 分"><time datetime="2017-07-05T12:38:25+08:00">1 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/27696130"><!-- react-text: 745 -->52<!-- /react-text --><!-- react-text: 746 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/27696130#comments"><!-- react-text: 749 -->17<!-- /react-text --><!-- react-text: 750 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/27629294" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-6f589df38509d14f839737645322a011_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/27629294"><span class="PostListItem-title">学会学习Learning to Learn：让AI拥有核心价值观从而实现快速学习</span><p class="PostListItem-summary"><!-- react-text: 759 -->说明：本文为智能单元专栏第一篇作者自产Paper的Blog！1 前言从2012年深度学习爆发以来，深度学习已经在计算机视觉，语音识别、翻译，游戏还有围棋等领域取得了革命性的突…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 761 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 6月 30 日星期五下午 5 点 28 分"><time datetime="2017-06-30T17:28:37+08:00">1 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/27629294"><!-- react-text: 771 -->111<!-- /react-text --><!-- react-text: 772 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/27629294#comments"><!-- react-text: 775 -->49<!-- /react-text --><!-- react-text: 776 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/27483965" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-f25c26025a9ea38acb63209f79644467_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/27483965"><span class="PostListItem-title">亲手教的AI总是放心些</span><p class="PostListItem-summary"><!-- react-text: 785 -->版权声明：本文智能单元首发，本人原创，禁止未授权转载。前言：6月12日，DeepMind和OpenAI联合发布了论文《Deep Reinforcement Learning from Human Preferences》，主要…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 787 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 6月 25 日星期日下午 4 点 29 分"><time datetime="2017-06-25T16:29:50+08:00">2 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/27483965"><!-- react-text: 797 -->87<!-- /react-text --><!-- react-text: 798 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/27483965#comments"><!-- react-text: 801 -->9<!-- /react-text --><!-- react-text: 802 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/26988866" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-3ce09841f5ec27b18afa2f741512b78e_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/26988866"><span class="PostListItem-title">最前沿：机器人学习Robot Learning的发展</span><p class="PostListItem-summary"><!-- react-text: 811 -->1 前言几天前，也就是5月16号，OpenAI发了一篇Blog介绍他们最新的机器人进展：Robots that Learn 他们的机器人已经可以只看一眼人类的行为就能模仿，虽然说他们的堆积木问…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 813 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 5月 19 日星期五下午 2 点 24 分"><time datetime="2017-05-19T14:24:35+08:00">3 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/26988866"><!-- react-text: 823 -->172<!-- /react-text --><!-- react-text: 824 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/26988866#comments"><!-- react-text: 827 -->15<!-- /react-text --><!-- react-text: 828 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/26882898" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-3ce09841f5ec27b18afa2f741512b78e_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/26882898"><span class="PostListItem-title">DRL之Policy Gradient, Deterministic Policy Gradient与Actor Critic</span><p class="PostListItem-summary"><!-- react-text: 837 -->1 前言首先对关注专栏的知友们表示抱歉。很不好意思，在上一篇DRL文章 深度增强学习之Policy Gradient方法1 - 知乎专栏 之后延迟了这么久的时间。中间忙了很多事，也对专栏…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 839 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 5月 13 日星期六下午 4 点 20 分"><time datetime="2017-05-13T16:20:01+08:00">3 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/26882898"><!-- react-text: 849 -->65<!-- /react-text --><!-- react-text: 850 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/26882898#comments"><!-- react-text: 853 -->22<!-- /react-text --><!-- react-text: 854 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/25302079" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-3ce09841f5ec27b18afa2f741512b78e_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/25302079"><span class="PostListItem-title">深度增强学习前沿算法思想</span><p class="PostListItem-summary"><!-- react-text: 863 -->本文原载于《程序员》杂志2017年1月刊2016年AlphaGo计算机围棋系统战胜顶尖职业棋手李世石，引起了全世界的广泛关注，人工智能进一步被推到了风口浪尖。而其中的深度增强学…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 865 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 2月 18 日星期六下午 1 点 10 分"><time datetime="2017-02-18T13:10:26+08:00">6 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/25302079"><!-- react-text: 875 -->270<!-- /react-text --><!-- react-text: 876 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/25302079#comments"><!-- react-text: 879 -->18<!-- /react-text --><!-- react-text: 880 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><!-- react-text: 883 --><!-- /react-text --><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/24996278"><span class="PostListItem-title">吴恩达对于增强学习的形象论述（下）</span><p class="PostListItem-summary"><!-- react-text: 888 -->版权声明：本文智能单元首发，本人原创，禁止未授权转载。前言：吴恩达在2003年为完成博士学位要求做了专题论文：Shaping and policy search in Reinforcement learning，其第一、二章被伯…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 890 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 2月 10 日星期五下午 5 点 15 分"><time datetime="2017-02-10T17:15:36+08:00">6 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/24996278"><!-- react-text: 900 -->127<!-- /react-text --><!-- react-text: 901 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/24996278#comments"><!-- react-text: 904 -->8<!-- /react-text --><!-- react-text: 905 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/25027944" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-fc658b76b2f0ed0c863953573cd5f462_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/25027944"><span class="PostListItem-title">逻辑与神经之间的桥 (2.0版)</span><p class="PostListItem-summary"><!-- react-text: 914 -->之前发表过了，但这次大修改后，有更实质的结论。（注： 这篇的内容建立在《游荡在思考的迷宫中》的架构上）<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 916 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/Cybernetic1" target="_blank">甄景贤</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 1月 26 日星期四下午 5 点 26 分"><time datetime="2017-01-26T17:26:58+08:00">6 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/25027944"><!-- react-text: 926 -->59<!-- /react-text --><!-- react-text: 927 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/25027944#comments"><!-- react-text: 930 -->22<!-- /react-text --><!-- react-text: 931 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/25027693" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-3ce09841f5ec27b18afa2f741512b78e_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/25027693"><span class="PostListItem-title">最前沿：基于GAN和RL的思想来训练对话生成，通过图灵测试可期！</span><p class="PostListItem-summary"><!-- react-text: 940 -->PS：本文分析略深，需要一定的RL和GAN的基础。前两天，Stanford的NLP小组出了一篇神经网络对话生成的论文：原文链接：https://arxiv.org/pdf/1701.06547.pdf标题就是使用对…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 942 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 1月 26 日星期四下午 4 点 44 分"><time datetime="2017-01-26T16:44:01+08:00">6 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/25027693"><!-- react-text: 952 -->140<!-- /react-text --><!-- react-text: 953 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/25027693#comments"><!-- react-text: 956 -->17<!-- /react-text --><!-- react-text: 957 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/24805121" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-56db09cd10a86d006257d0ff386dc2f7_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/24805121"><span class="PostListItem-title">汉字生成模型的那些坑</span><p class="PostListItem-summary"><!-- react-text: 966 -->引言人工智能技术目前仍处于技术积累期，人们真正希望看到的人工智能技术是要能够融入生活中的，真正为人们带来便利的技术，但目前能真正做到“实用化”的人工智能产品并不…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 968 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/lonely.wm" target="_blank">Lonely.wm</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 1月 23 日星期一中午 12 点 21 分"><time datetime="2017-01-23T12:21:15+08:00">7 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/24805121"><!-- react-text: 978 -->205<!-- /react-text --><!-- react-text: 979 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/24805121#comments"><!-- react-text: 982 -->17<!-- /react-text --><!-- react-text: 983 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/24977176" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-3ce09841f5ec27b18afa2f741512b78e_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/24977176"><span class="PostListItem-title">Q&amp;A 知乎Live：从AlphaGo看人工智能前沿技术</span><p class="PostListItem-summary"><!-- react-text: 992 -->1 前言在昨晚的 知乎Live：从AlphaGo看人工智能前沿技术 中，大家提了很多问题，由于Live时间的限制，来不及一一回复。这里将一些在Live中没有回答到的问题回答在这里。2 Q…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 994 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 1月 21 日星期六中午 11 点 37 分"><time datetime="2017-01-21T11:37:47+08:00">7 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/24977176"><!-- react-text: 1004 -->49<!-- /react-text --><!-- react-text: 1005 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/24977176#comments"><!-- react-text: 1008 -->7<!-- /react-text --><!-- react-text: 1009 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/24816290" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-3ce09841f5ec27b18afa2f741512b78e_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/24816290"><span class="PostListItem-title">Flood Sung的Live--从AlphaGo看人工智能前沿技术</span><p class="PostListItem-summary"><!-- react-text: 1018 -->大家好！我是 Flood Sung ，研究通用人工智能与机器人学习，「智能单元」知乎专栏作者之一。        深度增强学习（ Deep Reinforcement Learning ） 2013 年就被 DeepMind…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1020 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 1月 10 日星期二晚上 6 点 26 分"><time datetime="2017-01-10T18:26:59+08:00">7 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/24816290"><!-- react-text: 1030 -->27<!-- /react-text --><!-- react-text: 1031 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/24816290#comments"><!-- react-text: 1034 -->1<!-- /react-text --><!-- react-text: 1035 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><!-- react-text: 1038 --><!-- /react-text --><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/24761972"><span class="PostListItem-title">吴恩达对于增强学习的形象论述（上）</span><p class="PostListItem-summary"><!-- react-text: 1043 -->版权声明：本文智能单元首发，本人原创，禁止未授权转载。前言：吴恩达在2003年为完成博士学位要求做了专题论文：Shaping and policy search in Reinforcement learning，其第一、二章被伯…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1045 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 1月 9 日星期一上午 11 点 24 分"><time datetime="2017-01-09T11:24:14+08:00">7 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/24761972"><!-- react-text: 1055 -->183<!-- /react-text --><!-- react-text: 1056 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/24761972#comments"><!-- react-text: 1059 -->11<!-- /react-text --><!-- react-text: 1060 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><!-- react-text: 1063 --><!-- /react-text --><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/24721292"><span class="PostListItem-title">CS 294：深度增强学习，2017年春季学期</span><p class="PostListItem-summary"><!-- react-text: 1068 -->译者注：本文编译自伯克利大学2017年春季学期课程CS294介绍页面。该课程主题选择深度增强学习，即紧跟当前人工智能研究的热点，又可作为深度学习的后续方向，值得推荐。课程时间2017年1月18…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1070 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 1月 5 日星期四上午 10 点 26 分"><time datetime="2017-01-05T10:26:25+08:00">7 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/24721292"><!-- react-text: 1080 -->565<!-- /react-text --><!-- react-text: 1081 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/24721292#comments"><!-- react-text: 1084 -->55<!-- /react-text --><!-- react-text: 1085 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/24709235" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-eab9cfb4a3d814feb1e92cc1b69db9aa_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/24709235"><span class="PostListItem-title">Master 横扫围棋各路高手，是时候全面研究通用人工智能了！</span><p class="PostListItem-summary"><!-- react-text: 1094 -->在写这篇文章的时候，聂卫平将挑战Master，不出意外，老聂也只能扑街。有的人说，汽车早就比人跑得快，人也不会惊慌，围棋AI战胜人类，也是迟早的事，同样不必惊慌。是这样…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1096 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 1月 4 日星期三下午 3 点 05 分"><time datetime="2017-01-04T15:05:39+08:00">7 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/24709235"><!-- react-text: 1106 -->137<!-- /react-text --><!-- react-text: 1107 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/24709235#comments"><!-- react-text: 1110 -->133<!-- /react-text --><!-- react-text: 1111 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/23807875" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-a583638767b7ef5d44ddc68d748af3a5_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/23807875"><span class="PostListItem-title">ICLR 2017 DRL相关论文</span><p class="PostListItem-summary"><!-- react-text: 1120 -->1 前言ICLR 2017中和Deep Reinforcement Learning相关的论文我这边收集了一下，一共有30篇（可能有漏），大部分来自于DeepMind和OpenAI，可见DRL依然主要由DeepMind和OpenA…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1122 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 1月 3 日星期二中午 11 点 54 分"><time datetime="2017-01-03T11:54:32+08:00">7 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/23807875"><!-- react-text: 1132 -->160<!-- /react-text --><!-- react-text: 1133 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/23807875#comments"><!-- react-text: 1136 -->3<!-- /react-text --><!-- react-text: 1137 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/24682204" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-8b8c855409438a124291382551cc0cac_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/24682204"><span class="PostListItem-title">干货和原创的智能单元微信公众号开启</span><p class="PostListItem-summary"><!-- react-text: 1149 -->各位知友2017新年快乐！在新的一年，我们推出智能单元微信公众号，提供有别于知乎专栏的差异化优质内容。智能单元微信公众号智能单元微信公众号是一个致力于推动通用人工智…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1151 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2017 年 1月 3 日星期二上午 11 点 27 分"><time datetime="2017-01-03T11:27:11+08:00">7 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/24682204"><!-- react-text: 1161 -->21<!-- /react-text --><!-- react-text: 1162 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/24682204#comments"><!-- react-text: 1165 -->11<!-- /react-text --><!-- react-text: 1166 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/23080129" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-6f589df38509d14f839737645322a011_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/23080129"><span class="PostListItem-title">深度学习论文阅读路线图 Deep Learning Papers Reading Roadmap</span><p class="PostListItem-summary"><!-- react-text: 1175 -->1 前言相信很多想入门深度学习的朋友都会遇到这个问题，就是应该看哪些论文。包括我自己，也是花费了大量的时间在寻找文章上。另一方面，对于一些已经入门的朋友，常常也需…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1177 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 10月 20 日星期四中午 12 点 12 分"><time datetime="2016-10-20T12:12:51+08:00">10 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/23080129"><!-- react-text: 1187 -->1059<!-- /react-text --><!-- react-text: 1188 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/23080129#comments"><!-- react-text: 1191 -->41<!-- /react-text --><!-- react-text: 1192 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22888385" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-b1c917b1f2616bc51c7d833fdcc0c05d_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22888385"><span class="PostListItem-title">深层学习为何要“Deep”（上）</span><p class="PostListItem-summary"><!-- react-text: 1201 -->介绍为了研究神经网络，我们必须要对什么网络是什么有一个更直观的认识。一、基本变换：层神经网络是由一层一层构建的，那么每层究竟在做什么？数学式子：\vec{y}= a(W\cdo…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1203 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/YJango" target="_blank">YJango</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 10月 12 日星期三凌晨 2 点 33 分"><time datetime="2016-10-12T02:33:48+08:00">10 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22888385"><!-- react-text: 1213 -->898<!-- /react-text --><!-- react-text: 1214 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22888385#comments"><!-- react-text: 1217 -->76<!-- /react-text --><!-- react-text: 1218 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22457562" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/fc658b76b2f0ed0c863953573cd5f462_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22457562"><span class="PostListItem-title">逻辑与神经之间的桥</span><p class="PostListItem-summary"><!-- react-text: 1227 -->这是我迄今为止最有成果的论文，欢迎讨论！<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1229 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/Cybernetic1" target="_blank">甄景贤</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 9月 15 日星期四下午 12 点 55 分"><time datetime="2016-09-15T12:55:35+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22457562"><!-- react-text: 1239 -->179<!-- /react-text --><!-- react-text: 1240 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22457562#comments"><!-- react-text: 1243 -->63<!-- /react-text --><!-- react-text: 1244 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22758556" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-6f589df38509d14f839737645322a011_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22758556"><span class="PostListItem-title">最前沿 之 谷歌的协作机械臂</span><p class="PostListItem-summary"><!-- react-text: 1253 -->本文由我和李艺颖共同撰写。1 前言前几天也就是2016年10月3号，Google Research Blog上发表了最新的Blog，介绍他们在机器人上的工作：https://research.googleblog.com/201…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1255 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 10月 7 日星期五下午 4 点 39 分"><time datetime="2016-10-07T16:39:58+08:00">10 个月前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22758556"><!-- react-text: 1265 -->253<!-- /react-text --><!-- react-text: 1266 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22758556#comments"><!-- react-text: 1269 -->24<!-- /react-text --><!-- react-text: 1270 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22604627" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-9b132a69e08ced90ec418e66699b2c3a_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22604627"><span class="PostListItem-title">最前沿：围棋之后，AI玩FPS游戏也能秀人类一脸了！</span><p class="PostListItem-summary"><!-- react-text: 1279 -->版权声明：本文智能单元首发，本人原创，禁止未授权转载。前言：9月23日，基于经典第一人人称射击游戏毁灭战士DOOM的AI挑战赛“Visual Doom AI Competition @ CIG 2016”尘…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1281 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 9月 24 日星期六晚上 6 点 59 分"><time datetime="2016-09-24T18:59:40+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22604627"><!-- react-text: 1291 -->366<!-- /react-text --><!-- react-text: 1292 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22604627#comments"><!-- react-text: 1295 -->91<!-- /react-text --><!-- react-text: 1296 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22520434" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-7c2ef2d9f4d9244473a201b0b19318ec_b.jpeg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22520434"><span class="PostListItem-title">看图说话的AI小朋友——图像标注趣谈（下）</span><p class="PostListItem-summary"><!-- react-text: 1305 -->版权声明：本文智能单元首发，本人原创，禁止未授权转载。前言：近来图像标注（Image Caption）问题的研究热度渐高。本文希望在把问题和研究介绍清楚的同时行文通俗有趣，…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1307 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 9月 22 日星期四下午 4 点 40 分"><time datetime="2016-09-22T16:40:31+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22520434"><!-- react-text: 1317 -->139<!-- /react-text --><!-- react-text: 1318 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22520434#comments"><!-- react-text: 1321 -->21<!-- /react-text --><!-- react-text: 1322 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22523121" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/v2-6f589df38509d14f839737645322a011_b.jpeg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22523121"><span class="PostListItem-title">最前沿：深度增强学习再发力，家用机器人已近在眼前</span><p class="PostListItem-summary"><!-- react-text: 1331 -->1 前言如果问你“家用机器人真正实用化还需要多久？”我说的是那种可以端茶倒水，帮你干这干那的机器人，可能你的回答是10年，15年。但是现在，深度增强学习的不断发展，很…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1333 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 9月 21 日星期三下午 4 点 09 分"><time datetime="2016-09-21T16:09:25+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22523121"><!-- react-text: 1343 -->403<!-- /react-text --><!-- react-text: 1344 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22523121#comments"><!-- react-text: 1347 -->56<!-- /react-text --><!-- react-text: 1348 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22408033" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/7c2ef2d9f4d9244473a201b0b19318ec_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22408033"><span class="PostListItem-title">看图说话的AI小朋友——图像标注趣谈（上）</span><p class="PostListItem-summary"><!-- react-text: 1357 -->版权声明：本文智能单元首发，本人原创，禁止未授权转载。前言：近来图像标注（Image Caption）问题的研究热度渐高。本文希望在把问题和研究介绍清楚的同时行文通俗有趣，…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1359 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 9月 20 日星期二中午 12 点 04 分"><time datetime="2016-09-20T12:04:02+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22408033"><!-- react-text: 1369 -->170<!-- /react-text --><!-- react-text: 1370 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22408033#comments"><!-- react-text: 1373 -->26<!-- /react-text --><!-- react-text: 1374 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22230074" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/2029a1ea020883b4d065592d14acc96e_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22230074"><span class="PostListItem-title">[原创翻译]循环神经网络惊人的有效性（下）</span><p class="PostListItem-summary"><!-- react-text: 1383 -->版权声明：本文智能单元首发，本人原创翻译，禁止未授权转载。 译者注：在CS231n课程笔记止步于CNN，没有循环神经网络（RNN和LSTM），实为憾事。经知友推荐，将The Unreaso…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1385 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 9月 13 日星期二晚上 7 点 09 分"><time datetime="2016-09-13T19:09:57+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22230074"><!-- react-text: 1395 -->248<!-- /react-text --><!-- react-text: 1396 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22230074#comments"><!-- react-text: 1399 -->22<!-- /react-text --><!-- react-text: 1400 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21369441" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/6f589df38509d14f839737645322a011_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21369441"><span class="PostListItem-title">深度增强学习暑期学校 PPT 详解1</span><p class="PostListItem-summary"><!-- react-text: 1409 -->这是2016年机器学习暑期学校的课程，OpenAI的John Schulman做了4次Deep Reinforcement Learning的讲座，讲座的具体链接如下：Lecture 1: intro, derivative free optimizat…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1411 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 9月 11 日星期日下午 3 点 25 分"><time datetime="2016-09-11T15:25:05+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21369441"><!-- react-text: 1421 -->615<!-- /react-text --><!-- react-text: 1422 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21369441#comments"><!-- react-text: 1425 -->18<!-- /react-text --><!-- react-text: 1426 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22308032" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/cb5e078e5008907cb04b300369b7d621_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22308032"><span class="PostListItem-title">【总结】图像语义分割之FCN和CRF</span><p class="PostListItem-summary"><!-- react-text: 1435 -->前言 
(呕血制作啊！)前几天刚好做了个图像语义分割的汇报，把最近看的论文和一些想法讲了一下。所以今天就把它总结成文章啦，方便大家一起讨论讨论。本文只是展示了一些比…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1437 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/yu-chang-qian" target="_blank">ycszen</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 9月 4 日星期日晚上 9 点 55 分"><time datetime="2016-09-04T21:55:08+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22308032"><!-- react-text: 1447 -->420<!-- /react-text --><!-- react-text: 1448 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22308032#comments"><!-- react-text: 1451 -->76<!-- /react-text --><!-- react-text: 1452 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22107715" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/68ada38c72be590e4d3da1fbc7a26b65_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22107715"><span class="PostListItem-title">[原创翻译]循环神经网络惊人的有效性（上）</span><p class="PostListItem-summary"><!-- react-text: 1461 -->版权声明：本文智能单元首发，本人原创翻译，禁止未授权转载。 译者注：经知友推荐，将The Unreasonable Effectiveness of Recurrent Neural Networks一文翻译作为CS231n课…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1463 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 9月 6 日星期二晚上 8 点 31 分"><time datetime="2016-09-06T20:31:35+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22107715"><!-- react-text: 1473 -->384<!-- /react-text --><!-- react-text: 1474 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22107715#comments"><!-- react-text: 1477 -->47<!-- /react-text --><!-- react-text: 1478 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22282421" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/1b4d5e3761721c42438fbee54ccc8696_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22282421"><span class="PostListItem-title">Andrej Karpathy的回信和Quora活动邀请</span><p class="PostListItem-summary"><!-- react-text: 1487 -->大家好：关注我们CS231n翻译项目的知友应该知道，之所以开始这个翻译，也是因为得到了Andrej Karpathy的授权。这次翻译项目完结邮件了他，顺便问他有没有什么对学习CS231n…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1489 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 9月 4 日星期日晚上 10 点 36 分"><time datetime="2016-09-04T22:36:10+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22282421"><!-- react-text: 1499 -->87<!-- /react-text --><!-- react-text: 1500 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22282421#comments"><!-- react-text: 1503 -->23<!-- /react-text --><!-- react-text: 1504 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22232836" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/6e060188cc1571a89456af45670d51a9_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22232836"><span class="PostListItem-title">知行合一码作业，深度学习真入门</span><p class="PostListItem-summary"><!-- react-text: 1513 -->版权声明：文章为本人在智能单元专栏原创首发，禁止未授权转载。前言这是一篇号召深度学习爱好者共同学习、讨论和完成斯坦福深度学习课程CS231n的3个编程大作业的召集令。…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1515 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 8月 31 日星期三下午 2 点 51 分"><time datetime="2016-08-31T14:51:35+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22232836"><!-- react-text: 1525 -->524<!-- /react-text --><!-- react-text: 1526 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22232836#comments"><!-- react-text: 1529 -->51<!-- /react-text --><!-- react-text: 1530 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22143664" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/6f589df38509d14f839737645322a011_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22143664"><span class="PostListItem-title">最前沿：深度学习训练方法大革新，反向传播训练不再唯一</span><p class="PostListItem-summary"><!-- react-text: 1539 -->1 前言众所周知，在深度学习中我们使用反向传播算法进行训练。可能在任意一门深度学习课程中，反向传播都是必学的内容。我们使用反向传播计算每个参数的梯度，从而能够使用…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1541 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 8月 27 日星期六下午 3 点 59 分"><time datetime="2016-08-27T15:59:24+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22143664"><!-- react-text: 1551 -->742<!-- /react-text --><!-- react-text: 1552 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22143664#comments"><!-- react-text: 1555 -->60<!-- /react-text --><!-- react-text: 1556 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21930884" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/0dee7274beed25bd4abbcd76cb7d9576_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21930884"><span class="PostListItem-title">贺完结！CS231n官方笔记授权翻译总集篇发布</span><p class="PostListItem-summary"><!-- react-text: 1565 -->哈哈哈！我们也是不谦虚，几个“业余水平”的网友，怎么就“零星”地把这件事给搞完了呢！总之就是非常开心，废话不多说，进入正题吧！CS231n简介CS231n的全称是CS231n: Co…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1567 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 8月 25 日星期四下午 3 点 47 分"><time datetime="2016-08-25T15:47:00+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21930884"><!-- react-text: 1577 -->1099<!-- /react-text --><!-- react-text: 1578 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21930884#comments"><!-- react-text: 1581 -->161<!-- /react-text --><!-- react-text: 1582 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/22038289" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/5b83bc8331994f47fedb1459d1424872_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/22038289"><span class="PostListItem-title">CS231n课程笔记翻译：卷积神经网络笔记</span><p class="PostListItem-summary"><!-- react-text: 1591 -->译者注：本文翻译自斯坦福CS231n课程笔记ConvNet notes，由课程教师Andrej Karpathy授权进行翻译。本篇教程由杜客和猴子翻译完成，堃堃和李艺颖进行校对修改。原文如下内容…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1593 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/hmonkey" target="_blank">猴子</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 8月 20 日星期六中午 12 点 24 分"><time datetime="2016-08-20T12:24:22+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/22038289"><!-- react-text: 1603 -->436<!-- /react-text --><!-- react-text: 1604 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/22038289#comments"><!-- react-text: 1607 -->75<!-- /react-text --><!-- react-text: 1608 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21341440" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/3db4d60c513ffde3c66eb745bbf36f99_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21341440"><span class="PostListItem-title">「无中生有」计算机视觉探奇</span><p class="PostListItem-summary"><!-- react-text: 1617 -->计算机视觉 (Computer Vision, CV) 是一门研究如何使机器“看”的科学。1963年来自MIT的Larry Roberts发表的该领域第一篇博士论文“Machine Perception of Three-Dimension…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1619 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/wei-xiu-shen" target="_blank">魏秀参</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 7月 4 日星期一晚上 8 点 32 分"><time datetime="2016-07-04T20:32:07+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21341440"><!-- react-text: 1629 -->312<!-- /react-text --><!-- react-text: 1630 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21341440#comments"><!-- react-text: 1633 -->21<!-- /react-text --><!-- react-text: 1634 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21917736" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/7d2d39eb9ae721a13607713c73193172_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21917736"><span class="PostListItem-title">智能单元专栏投稿说明</span><p class="PostListItem-summary"><!-- react-text: 1643 -->首先感谢各位知友对于本专栏的支持！发布本说明是因为收到知友的投稿和建议，促使我和@Flood Sung 重新思考专栏的定位和意义。来龙去脉原本定位：这个专栏原本只是我们整理…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1645 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 8月 14 日星期日晚上 9 点 30 分"><time datetime="2016-08-14T21:30:21+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21917736"><!-- react-text: 1655 -->34<!-- /react-text --><!-- react-text: 1656 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21917736#comments"><!-- react-text: 1659 -->7<!-- /react-text --><!-- react-text: 1660 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21946525" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/89758df295a0927d26da59356338a2ff_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21946525"><span class="PostListItem-title">斯坦福CS231n课程作业# 3简介</span><p class="PostListItem-summary"><!-- react-text: 1669 -->译者注：本文智能单元首发，由@杜客翻译自斯坦福CS231n课程作业1介绍页面[Assignment #2]。原文如下在本作业中，你将实现循环网络，并将其应用于在微软的COCO数据库上进行…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1671 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 8月 11 日星期四早上 6 点 30 分"><time datetime="2016-08-11T06:30:36+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21946525"><!-- react-text: 1681 -->57<!-- /react-text --><!-- react-text: 1682 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21946525#comments"><!-- react-text: 1685 -->29<!-- /react-text --><!-- react-text: 1686 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21941485" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/4b4c825806caf0c939f97a95be32944c_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21941485"><span class="PostListItem-title">斯坦福CS231n课程作业# 2简介</span><p class="PostListItem-summary"><!-- react-text: 1695 -->译者注：本文智能单元首发，由@杜客翻译自斯坦福CS231n课程作业1介绍页面[Assignment #2]。原文如下在本作业中，你将练习编写反向传播代码，训练神经网络和卷积神经网络。…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1697 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 8月 10 日星期三上午 11 点 19 分"><time datetime="2016-08-10T11:19:25+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21941485"><!-- react-text: 1707 -->78<!-- /react-text --><!-- react-text: 1708 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21941485#comments"><!-- react-text: 1711 -->2<!-- /react-text --><!-- react-text: 1712 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21798784" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/940c2e8d2edc3018771c752d977a6f27_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21798784"><span class="PostListItem-title">CS231n课程笔记翻译：神经网络笔记3（下）</span><p class="PostListItem-summary"><!-- react-text: 1721 -->译者注：本文智能单元首发，译自斯坦福CS231n课程笔记Neural Nets notes 3，课程教师Andrej Karpathy授权翻译。本篇教程由杜客翻译完成，堃堃和巩子嘉进行校对修改。译文含…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1723 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 8月 9 日星期二下午 3 点 12 分"><time datetime="2016-08-09T15:12:17+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21798784"><!-- react-text: 1733 -->91<!-- /react-text --><!-- react-text: 1734 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21798784#comments"><!-- react-text: 1737 -->22<!-- /react-text --><!-- react-text: 1738 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21725498" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/6f589df38509d14f839737645322a011_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21725498"><span class="PostListItem-title">深度增强学习之Policy Gradient方法1</span><p class="PostListItem-summary"><!-- react-text: 1747 -->1 前言在之前的深度增强学习系列文章中，我们已经详细分析了DQN算法，一种基于价值Value的算法，那么在今天，我们和大家一起分析深度增强学习中的另一种算法，也就是基于策…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1749 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 8月 3 日星期三上午 11 点 04 分"><time datetime="2016-08-03T11:04:25+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21725498"><!-- react-text: 1759 -->128<!-- /react-text --><!-- react-text: 1760 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21725498#comments"><!-- react-text: 1763 -->17<!-- /react-text --><!-- react-text: 1764 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21741716" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/b629f243297cf32d2507bdaa1bc38e12_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21741716"><span class="PostListItem-title">CS231n课程笔记翻译：神经网络笔记3（上）</span><p class="PostListItem-summary"><!-- react-text: 1773 -->译者注：本文智能单元首发，译自斯坦福CS231n课程笔记Neural Nets notes 3，课程教师Andrej Karpathy授权翻译。本篇教程由杜客翻译完成，堃堃和巩子嘉进行校对修改。译文含…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1775 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 8月 2 日星期二下午 2 点 26 分"><time datetime="2016-08-02T14:26:25+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21741716"><!-- react-text: 1785 -->120<!-- /react-text --><!-- react-text: 1786 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21741716#comments"><!-- react-text: 1789 -->42<!-- /react-text --><!-- react-text: 1790 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21560667" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/9843b69865ebb95506dfe9b4df48e31c_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21560667"><span class="PostListItem-title">CS231n课程笔记翻译：神经网络笔记 2</span><p class="PostListItem-summary"><!-- react-text: 1799 -->译者注：本文智能单元首发，译自斯坦福CS231n课程笔记Neural Nets notes 2，课程教师Andrej Karpathy授权翻译。本篇教程由杜客翻译完成，堃堃进行校对修改。译文含公式和代…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1801 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 7月 26 日星期二下午 5 点 19 分"><time datetime="2016-07-26T17:19:34+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21560667"><!-- react-text: 1811 -->199<!-- /react-text --><!-- react-text: 1812 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21560667#comments"><!-- react-text: 1815 -->53<!-- /react-text --><!-- react-text: 1816 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21609472" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/178c11fcc99098e7fc039c8f7a96576d_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21609472"><span class="PostListItem-title">DQN从入门到放弃7  连续控制DQN算法-NAF</span><p class="PostListItem-summary"><!-- react-text: 1825 -->1 前言在上一篇文章DQN从入门到放弃6 DQN的各种改进中，我们介绍了DQN的各个方面的改进。从各种改进的角度和思路很有利于我们思考如何去创新这个事情。那么，本着从入门到…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1827 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 7月 15 日星期五晚上 11 点 38 分"><time datetime="2016-07-15T23:38:37+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21609472"><!-- react-text: 1837 -->54<!-- /react-text --><!-- react-text: 1838 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21609472#comments"><!-- react-text: 1841 -->21<!-- /react-text --><!-- react-text: 1842 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21513367" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/421c2492bfe445a3b7e92e18ced0e8dd_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21513367"><span class="PostListItem-title">CS231n课程笔记翻译：神经网络笔记1（下）</span><p class="PostListItem-summary"><!-- react-text: 1851 -->译者注：本文智能单元首发，译自斯坦福CS231n课程笔记Neural Nets notes 1，课程教师Andrej Karpathy授权翻译。本篇教程由杜客翻译完成，李艺颖和堃堃进行校对修改。译文含…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1853 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 7月 10 日星期日下午 5 点 33 分"><time datetime="2016-07-10T17:33:42+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21513367"><!-- react-text: 1863 -->213<!-- /react-text --><!-- react-text: 1864 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21513367#comments"><!-- react-text: 1867 -->32<!-- /react-text --><!-- react-text: 1868 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21547911" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/178c11fcc99098e7fc039c8f7a96576d_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21547911"><span class="PostListItem-title">DQN从入门到放弃6 DQN的各种改进</span><p class="PostListItem-summary"><!-- react-text: 1877 -->1 前言在上一篇文章DQN从入门到放弃5 深度解读DQN算法中，我们深入地介绍了基本的DQN算法，也就是NIPS 2013版本的算法。那么在这之后，DeepMind不断对DQN进行改进，首先在2…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1879 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 7月 10 日星期日下午 5 点 00 分"><time datetime="2016-07-10T17:00:10+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21547911"><!-- react-text: 1889 -->95<!-- /react-text --><!-- react-text: 1890 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21547911#comments"><!-- react-text: 1893 -->33<!-- /react-text --><!-- react-text: 1894 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21462488" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/95e9b62e33724863ceed6ca6de68a835_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21462488"><span class="PostListItem-title">CS231n课程笔记翻译：神经网络笔记1（上）</span><p class="PostListItem-summary"><!-- react-text: 1903 -->译者注：本文智能单元首发，译自斯坦福CS231n课程笔记Neural Nets notes 1，课程教师Andrej Karpathy授权翻译。本篇教程由杜客翻译完成，巩子嘉和堃堃进行校对修改。译文含…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1905 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 7月 5 日星期二上午 11 点 03 分"><time datetime="2016-07-05T11:03:52+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21462488"><!-- react-text: 1915 -->222<!-- /react-text --><!-- react-text: 1916 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21462488#comments"><!-- react-text: 1919 -->43<!-- /react-text --><!-- react-text: 1920 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21470871" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/6f589df38509d14f839737645322a011_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21470871"><span class="PostListItem-title">最前沿：从虚拟到现实，迁移深度增强学习让机器人革命成为可能！</span><p class="PostListItem-summary"><!-- react-text: 1929 -->1 机器人革命？Are you joking？如果问我Google旗下的DeepMind会是一家怎样的公司的话，我的回答会是“DeepMind将可能成为一家世界上最牛逼的机器人公司！”因为，人工智能…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1931 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 7月 4 日星期一下午 12 点 52 分"><time datetime="2016-07-04T12:52:09+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21470871"><!-- react-text: 1941 -->552<!-- /react-text --><!-- react-text: 1942 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21470871#comments"><!-- react-text: 1945 -->89<!-- /react-text --><!-- react-text: 1946 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21477488" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/178c11fcc99098e7fc039c8f7a96576d_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21477488"><span class="PostListItem-title">150行代码实现DQN算法玩CartPole</span><p class="PostListItem-summary"><!-- react-text: 1955 -->1 前言终于到了DQN系列真正的实战了。今天我们将一步一步的告诉大家如何用最短的代码实现基本的DQN算法，并且完成基本的RL任务。这恐怕也将是你在网上能找到的最详尽的DQN…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1957 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 7月 3 日星期日下午 3 点 49 分"><time datetime="2016-07-03T15:49:41+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21477488"><!-- react-text: 1967 -->219<!-- /react-text --><!-- react-text: 1968 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21477488#comments"><!-- react-text: 1971 -->97<!-- /react-text --><!-- react-text: 1972 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21407711" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/6cfac1401554ece6dac25a7ef7f7fe79_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21407711"><span class="PostListItem-title">CS231n课程笔记翻译：反向传播笔记</span><p class="PostListItem-summary"><!-- react-text: 1981 -->译者注：本文智能单元首发，译自斯坦福CS231n课程笔记Backprop Note，课程教师Andrej Karpathy授权翻译。本篇教程由杜客翻译完成，堃堃和巩子嘉进行校对修改。译文含公式和…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 1983 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 29 日星期三晚上 7 点 41 分"><time datetime="2016-06-29T19:41:06+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21407711"><!-- react-text: 1993 -->424<!-- /react-text --><!-- react-text: 1994 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21407711#comments"><!-- react-text: 1997 -->66<!-- /react-text --><!-- react-text: 1998 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21441838" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/97126231dd86c8765c26b1cd587e8201_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21441838"><span class="PostListItem-title">斯坦福CS231n课程作业# 1简介</span><p class="PostListItem-summary"><!-- react-text: 2007 -->译者注：本文智能单元首发，由杜客翻译自斯坦福CS231n课程作业1介绍页面[Assignment #1]。该课程共有3个作业，建议深度学习入门的知友完成。原文如下在本作业中，将基于k-…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2009 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 28 日星期二晚上 6 点 59 分"><time datetime="2016-06-28T18:59:04+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21441838"><!-- react-text: 2019 -->140<!-- /react-text --><!-- react-text: 2020 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21441838#comments"><!-- react-text: 2023 -->54<!-- /react-text --><!-- react-text: 2024 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21434933" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/6f589df38509d14f839737645322a011_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21434933"><span class="PostListItem-title">DQN实战篇1 从零开始安装Ubuntu, Cuda, Cudnn, Tensorflow, OpenAI Gym</span><p class="PostListItem-summary"><!-- react-text: 2033 -->1 前言关注深度增强学习的知友们，大家好！之前我们专栏发布了 DQN从入门到放弃系列文章，从理论上为大家详细分析DQN的算法原理，至该系列文章第五篇，已经把DQN的整个算法…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2035 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 27 日星期一晚上 9 点 37 分"><time datetime="2016-06-27T21:37:23+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21434933"><!-- react-text: 2045 -->297<!-- /react-text --><!-- react-text: 2046 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21434933#comments"><!-- react-text: 2049 -->61<!-- /react-text --><!-- react-text: 2050 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21421729" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/178c11fcc99098e7fc039c8f7a96576d_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21421729"><span class="PostListItem-title">DQN从入门到放弃5 深度解读DQN算法</span><p class="PostListItem-summary"><!-- react-text: 2059 -->0 前言如果说DQN从入门到放弃的前四篇是开胃菜的话，那么本篇文章就是主菜了。所以，等吃完主菜再放弃吧！1 详解Q-Learning在上一篇文章DQN从入门到放弃 第四篇中，我们分…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2061 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 25 日星期六下午 4 点 47 分"><time datetime="2016-06-25T16:47:48+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21421729"><!-- react-text: 2071 -->152<!-- /react-text --><!-- react-text: 2072 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21421729#comments"><!-- react-text: 2075 -->43<!-- /react-text --><!-- react-text: 2076 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21387326" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/d43a9c7ffaef1eef1d3f0b434e38dd3b_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21387326"><span class="PostListItem-title">CS231n课程笔记翻译：最优化笔记（下）</span><p class="PostListItem-summary"><!-- react-text: 2085 -->译者注：本文智能单元首发，译自斯坦福CS231n课程笔记Optimization Note，课程教师Andrej Karpathy授权翻译。本篇教程由杜客翻译完成，李艺颖和堃堃进行校对修改。译文含公…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2087 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 23 日星期四晚上 6 点 33 分"><time datetime="2016-06-23T18:33:55+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21387326"><!-- react-text: 2097 -->120<!-- /react-text --><!-- react-text: 2098 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21387326#comments"><!-- react-text: 2101 -->35<!-- /react-text --><!-- react-text: 2102 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21392483" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/e7225f2c6cddb586ee4ba6f54018b55b_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21392483"><span class="PostListItem-title">知友Tess Lo关于共享Coursera旧课程资源的倡议</span><p class="PostListItem-summary"><!-- react-text: 2111 -->各位知友：大家好。在6月19日，知友Tess Lo 联系了我，告知了我关于Coursera因关闭旧平台，对部分旧课程资源废弃的情况。提出了通过我们专栏来号召有意愿的小伙伴合作下载…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2113 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 21 日星期二下午 4 点 53 分"><time datetime="2016-06-21T16:53:28+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21392483"><!-- react-text: 2123 -->198<!-- /react-text --><!-- react-text: 2124 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21392483#comments"><!-- react-text: 2127 -->17<!-- /react-text --><!-- react-text: 2128 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21360434" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/d43a9c7ffaef1eef1d3f0b434e38dd3b_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21360434"><span class="PostListItem-title">CS231n课程笔记翻译：最优化笔记（上）</span><p class="PostListItem-summary"><!-- react-text: 2137 -->译者注：本文智能单元首发，译自斯坦福CS231n课程笔记Optimization Note，课程教师Andrej Karpathy授权翻译。本篇教程由杜客翻译完成，堃堃和李艺颖进行校对修改。译文含公…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2139 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 21 日星期二下午 4 点 10 分"><time datetime="2016-06-21T16:10:44+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21360434"><!-- react-text: 2149 -->133<!-- /react-text --><!-- react-text: 2150 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21360434#comments"><!-- react-text: 2153 -->10<!-- /react-text --><!-- react-text: 2154 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21362413" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/6f589df38509d14f839737645322a011_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21362413"><span class="PostListItem-title">最前沿：让计算机学会学习Let Computers Learn to Learn</span><p class="PostListItem-summary"><!-- react-text: 2163 -->自从AlphaGo战胜了李世石之后，很多知友们肯定深感深度学习的厉害，觉得深度学习让计算机具备了学习能力，还能够自我学习提升。但是，深度学习真的会学习吗？我的意思是深…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2165 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 21 日星期二下午 1 点 25 分"><time datetime="2016-06-21T13:25:41+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21362413"><!-- react-text: 2175 -->1182<!-- /react-text --><!-- react-text: 2176 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21362413#comments"><!-- react-text: 2179 -->123<!-- /react-text --><!-- react-text: 2180 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21378532" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/178c11fcc99098e7fc039c8f7a96576d_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21378532"><span class="PostListItem-title">DQN 从入门到放弃4 动态规划与Q-Learning</span><p class="PostListItem-summary"><!-- react-text: 2189 -->1 上文回顾在上一篇文章DQN从入门到放弃 第三篇中，我们分析到了Bellman方程，其方程v(s) = \mathbb E[R_{t+1} + \lambda v(S_{t+1})|S_t = s]极其简洁，透出的含义就是价…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2191 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 19 日星期日上午 9 点 49 分"><time datetime="2016-06-19T09:49:24+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21378532"><!-- react-text: 2201 -->103<!-- /react-text --><!-- react-text: 2202 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21378532#comments"><!-- react-text: 2205 -->38<!-- /react-text --><!-- react-text: 2206 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21354230" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/df9a2603b2c05505733dd25b6d5a1b3a_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21354230"><span class="PostListItem-title">知友智靖远关于CS231n课程字幕翻译的倡议</span><p class="PostListItem-summary"><!-- react-text: 2215 -->正在学习CS231n的知友们：大家好！在CS231n课程笔记翻译：线性分类笔记（下）文末的特别感谢部分，我简要介绍了知友智靖远关于字幕翻译的贡献和倡议，下面将整个事情的来龙…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2217 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 14 日星期二晚上 9 点 52 分"><time datetime="2016-06-14T21:52:27+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21354230"><!-- react-text: 2227 -->70<!-- /react-text --><!-- react-text: 2228 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21354230#comments"><!-- react-text: 2231 -->19<!-- /react-text --><!-- react-text: 2232 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21102293" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/86b3f2e3cf390a8319a365846a0f39a8_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21102293"><span class="PostListItem-title">CS231n课程笔记翻译：线性分类笔记（下）</span><p class="PostListItem-summary"><!-- react-text: 2241 -->译者注：本文智能单元首发，译自斯坦福CS231n课程笔记Linear Classification Note，课程教师Andrej Karpathy授权翻译。本篇教程由杜客翻译完成，堃堃进行校对修改。译文含…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2243 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 14 日星期二晚上 7 点 18 分"><time datetime="2016-06-14T19:18:33+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21102293"><!-- react-text: 2253 -->85<!-- /react-text --><!-- react-text: 2254 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21102293#comments"><!-- react-text: 2257 -->36<!-- /react-text --><!-- react-text: 2258 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21320865" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/3f270779748e39720b998eb10f6e4359_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21320865"><span class="PostListItem-title">最前沿：史蒂夫的人工智能大挑战</span><p class="PostListItem-summary"><!-- react-text: 2267 -->版权声明：本文是Flood Sung和杜客合作的原创文章，未经授权禁止转载。Flood Sung：arXiv上有篇新论文，利用像素游戏我的世界来做深度增强学习的实验。杜客：有意思！搞一…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2269 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 14 日星期二早上 7 点 21 分"><time datetime="2016-06-14T07:21:27+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21320865"><!-- react-text: 2279 -->184<!-- /react-text --><!-- react-text: 2280 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21320865#comments"><!-- react-text: 2283 -->25<!-- /react-text --><!-- react-text: 2284 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21340755" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/178c11fcc99098e7fc039c8f7a96576d_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21340755"><span class="PostListItem-title">DQN 从入门到放弃3 价值函数与Bellman方程</span><p class="PostListItem-summary"><!-- react-text: 2293 -->1 上文回顾在上一篇文章DQN 从入门到放弃 第二篇中，我们探讨了增强学习问题的基本假设，然后引出了MDP马尔科夫决策过程。MDP只需要用一句话就可以说明白，就是“未来只取…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2295 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 12 日星期日晚上 10 点 49 分"><time datetime="2016-06-12T22:49:38+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21340755"><!-- react-text: 2305 -->80<!-- /react-text --><!-- react-text: 2306 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21340755#comments"><!-- react-text: 2309 -->25<!-- /react-text --><!-- react-text: 2310 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21340806" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/5a076004b641e2e4ba416336941bf2dc_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21340806"><span class="PostListItem-title">智能单元获得知乎赞赏内测资格</span><p class="PostListItem-summary"><!-- react-text: 2319 -->今天下午，收到了知乎专栏的私信：虽然之前得到了开通内测功能的通知，也有申请，但是没有抱任何希望。完全没有想到能够获得赞赏的内测资格，感到我们的工作被知乎肯定了，…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2321 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 12 日星期日晚上 10 点 48 分"><time datetime="2016-06-12T22:48:27+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21340806"><!-- react-text: 2331 -->39<!-- /react-text --><!-- react-text: 2332 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21340806#comments"><!-- react-text: 2335 -->7<!-- /react-text --><!-- react-text: 2336 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/20945670" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/86b3f2e3cf390a8319a365846a0f39a8_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/20945670"><span class="PostListItem-title">CS231n课程笔记翻译：线性分类笔记（中）</span><p class="PostListItem-summary"><!-- react-text: 2345 -->译者注：本文智能单元首发，译自斯坦福CS231n课程笔记Linear Classification Note，课程教师Andrej Karpathy授权翻译。本篇教程由杜客翻译完成，ShiqingFan和堃堃进行校对…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2347 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 7 日星期二下午 3 点 13 分"><time datetime="2016-06-07T15:13:15+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/20945670"><!-- react-text: 2357 -->148<!-- /react-text --><!-- react-text: 2358 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/20945670#comments"><!-- react-text: 2361 -->47<!-- /react-text --><!-- react-text: 2362 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21292697" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/ff207a009c636ccf4fe9103cbbac899c_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21292697"><span class="PostListItem-title">DQN 从入门到放弃2 增强学习与MDP</span><p class="PostListItem-summary"><!-- react-text: 2371 -->1 上文回顾在上一篇文章DQN 从入门到放弃 第一篇中，我们回答了三个问题：为什么研究深度增强学习？为什么研究DQN？什么是增强学习？那么在这一篇文章中，我们将进一步探讨…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2373 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 6 日星期一凌晨 12 点 14 分"><time datetime="2016-06-06T00:14:08+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21292697"><!-- react-text: 2383 -->114<!-- /react-text --><!-- react-text: 2384 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21292697#comments"><!-- react-text: 2387 -->29<!-- /react-text --><!-- react-text: 2388 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21296798" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/8acece694a6ae89631dc4df8b0587149_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21296798"><span class="PostListItem-title">解密Google Deepmind AlphaGo围棋算法：真人工智能来自于哪里？</span><p class="PostListItem-summary"><!-- react-text: 2397 -->版权声明：本文为原创文章，于2016年1月30号原载于本人的csdn博客：解密Google Deepmind AlphaGo围棋算法：真人工智能来自于哪里？由于保护不周，在未经允许下被多方转载。…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2399 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 6月 3 日星期五下午 4 点 39 分"><time datetime="2016-06-03T16:39:58+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21296798"><!-- react-text: 2409 -->88<!-- /react-text --><!-- react-text: 2410 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21296798#comments"><!-- react-text: 2413 -->19<!-- /react-text --><!-- react-text: 2414 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/20918580" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/86b3f2e3cf390a8319a365846a0f39a8_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/20918580"><span class="PostListItem-title">CS231n课程笔记翻译：线性分类笔记（上）</span><p class="PostListItem-summary"><!-- react-text: 2423 -->译者注：本文智能单元首发，译自斯坦福CS231n课程笔记Linear Classification Note，课程教师Andrej Karpathy授权翻译。本篇教程由杜客翻译完成，巩子嘉和堃堃进行校对修改…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2425 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 5月 31 日星期二晚上 7 点 21 分"><time datetime="2016-05-31T19:21:22+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/20918580"><!-- react-text: 2435 -->161<!-- /react-text --><!-- react-text: 2436 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/20918580#comments"><!-- react-text: 2439 -->40<!-- /react-text --><!-- react-text: 2440 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21263408" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/6f589df38509d14f839737645322a011_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21263408"><span class="PostListItem-title">你是这样获取人工智能AI前沿信息的吗？</span><p class="PostListItem-summary"><!-- react-text: 2449 -->版权声明：本文为原创文章，未经作者允许不得转载。前言对于Researchers或者Geeks而言，特别是并没有在顶级的科研圈里的人，如何高效的获取最新的科技前沿，对自己的研究方…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2451 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 5月 29 日星期日中午 12 点 11 分"><time datetime="2016-05-29T12:11:16+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21263408"><!-- react-text: 2461 -->432<!-- /react-text --><!-- react-text: 2462 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21263408#comments"><!-- react-text: 2465 -->23<!-- /react-text --><!-- react-text: 2466 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/21262246" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/ff207a009c636ccf4fe9103cbbac899c_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/21262246"><span class="PostListItem-title">DQN 从入门到放弃1 DQN与增强学习</span><p class="PostListItem-summary"><!-- react-text: 2475 -->1 前言深度增强学习Deep Reinforcement Learning是将深度学习与增强学习结合起来从而实现从Perception感知到Action动作的端对端学习End-to-End Learning的一种全新的算法。…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2477 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 5月 29 日星期日凌晨 1 点 01 分"><time datetime="2016-05-29T01:01:00+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/21262246"><!-- react-text: 2487 -->337<!-- /react-text --><!-- react-text: 2488 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/21262246#comments"><!-- react-text: 2491 -->31<!-- /react-text --><!-- react-text: 2492 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/20900216" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/4b149e4f05ca3551cadcb07c0963cb6a_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/20900216"><span class="PostListItem-title">CS231n课程笔记翻译：图像分类笔记（下）</span><p class="PostListItem-summary"><!-- react-text: 2501 -->译者注：本文智能单元首发，翻译自斯坦福CS231n课程笔记image classification notes，课程教师Andrej Karpathy授权翻译。本篇教程由杜客进行翻译，ShiqingFan和巩子嘉进行…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2503 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 5月 24 日星期二晚上 7 点 11 分"><time datetime="2016-05-24T19:11:48+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/20900216"><!-- react-text: 2513 -->133<!-- /react-text --><!-- react-text: 2514 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/20900216#comments"><!-- react-text: 2517 -->41<!-- /react-text --><!-- react-text: 2518 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/20924929" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/fc15566ed11c5708847994ee81e927bf_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/20924929"><span class="PostListItem-title">从OpenAI看深度学习研究前沿</span><p class="PostListItem-summary"><!-- react-text: 2527 -->版权说明：本文为原创文章，未经作者允许不得转载。1 前言想必很多知友都知道OpenAI这家初创公司。OpenAI是2015年底刚成立的人工智能公司，由Elon Musk领投，号称有10亿美…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2529 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 5月 18 日星期三晚上 8 点 51 分"><time datetime="2016-05-18T20:51:10+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/20924929"><!-- react-text: 2539 -->306<!-- /react-text --><!-- react-text: 2540 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/20924929#comments"><!-- react-text: 2543 -->25<!-- /react-text --><!-- react-text: 2544 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/20894041" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/4b149e4f05ca3551cadcb07c0963cb6a_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/20894041"><span class="PostListItem-title">CS231n课程笔记翻译：图像分类笔记（上）</span><p class="PostListItem-summary"><!-- react-text: 2553 -->译者注：本文智能单元首发，译自斯坦福CS231n课程笔记image classification notes，由课程教师Andrej Karpathy授权进行翻译。本篇教程由杜客翻译完成。ShiqingFan对译文进…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2555 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 5月 16 日星期一凌晨 12 点 30 分"><time datetime="2016-05-16T00:30:45+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/20894041"><!-- react-text: 2565 -->197<!-- /react-text --><!-- react-text: 2566 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/20894041#comments"><!-- react-text: 2569 -->55<!-- /react-text --><!-- react-text: 2570 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/20893777" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/286c0a4999fd07952446f1d7aa75794e_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/20893777"><span class="PostListItem-title">深度解读AlphaGo</span><p class="PostListItem-summary"><!-- react-text: 2579 -->AlphaGo战胜李世石可以说是人工智能领域里程碑式的事件，代表了人工智能水平的重要突破。而AlphaGo内部的核心就是使用深度增强学习技术。本ppt尝试从算法层面深入解读Alpha…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2581 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 5月 13 日星期五上午 11 点 08 分"><time datetime="2016-05-13T11:08:22+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/20893777"><!-- react-text: 2591 -->85<!-- /react-text --><!-- react-text: 2592 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/20893777#comments"><!-- react-text: 2595 -->16<!-- /react-text --><!-- react-text: 2596 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/20885568" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/6f589df38509d14f839737645322a011_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/20885568"><span class="PostListItem-title">Deep Reinforcement Learning 深度增强学习资源 (持续更新）</span><p class="PostListItem-summary"><!-- react-text: 2605 -->Deep Reinforcement Learning深度增强学习可以说发源于2013年DeepMind的Playing Atari with Deep Reinforcement Learning 一文，之后2015年DeepMind 在Nature上发表了Human…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2607 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/flood-sung" target="_blank">Flood Sung</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 5月 11 日星期三晚上 11 点 22 分"><time datetime="2016-05-11T23:22:44+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/20885568"><!-- react-text: 2617 -->630<!-- /react-text --><!-- react-text: 2618 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/20885568#comments"><!-- react-text: 2621 -->27<!-- /react-text --><!-- react-text: 2622 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/20878530" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/56fe9d4b74ec164413a54bc57889c480_b.jpg" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/20878530"><span class="PostListItem-title">CS231n课程笔记翻译：Python Numpy教程</span><p class="PostListItem-summary"><!-- react-text: 2631 -->译者注：本文智能单元首发，翻译自斯坦福CS231n课程笔记Python Numpy Tutorial，由课程教师Andrej Karpathy授权进行翻译。本篇教程由杜客翻译完成，Flood Sung、SunisDown…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2633 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 5月 11 日星期三晚上 9 点 00 分"><time datetime="2016-05-11T21:00:25+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/20878530"><!-- react-text: 2643 -->560<!-- /react-text --><!-- react-text: 2644 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/20878530#comments"><!-- react-text: 2647 -->89<!-- /react-text --><!-- react-text: 2648 --> 条评论<!-- /react-text --></a></span></div></div></div></li><li><div class="PostListItem PostListItem--responsive"><a href="https://zhuanlan.zhihu.com/p/20870307" class="PostListItem-titleImageWrapper"><img src="./智能单元 - 知乎专栏_files/f6c276dda4a9139b77b553f037ab79c4_b.png" class="PostListItem-titleImage" alt="题图"></a><div class="PostListItem-info"><a href="https://zhuanlan.zhihu.com/p/20870307"><span class="PostListItem-title">获得授权翻译斯坦福CS231n课程笔记系列</span><p class="PostListItem-summary"><!-- react-text: 2657 -->一切都要从这个回答说起：应该选择TensorFlow还是Theano？ - 杜客的回答斯坦福CS231n-Convolutional Neural Networks for Visual Recognition(Winter 2016)是一个很好的课…<!-- /react-text --><span class="PostListItem-readall"><!-- react-text: 2659 -->查看全文<!-- /react-text --><i class="icon icon-ic_unfold"></i></span></p></a><div class="PostListItem-footer"><span><a class="PostListItem-name" href="https://www.zhihu.com/people/du-ke" target="_blank">杜客</a><span class="Bull"></span><div class="HoverTitle PostListItem-date" data-hover-title="2016 年 5月 9 日星期一下午 4 点 53 分"><time datetime="2016-05-09T16:53:30+08:00">1 年前</time></div></span><span><a href="https://zhuanlan.zhihu.com/p/20870307"><!-- react-text: 2669 -->189<!-- /react-text --><!-- react-text: 2670 --> 赞<!-- /react-text --></a><span class="Bull"></span><a href="https://zhuanlan.zhihu.com/p/20870307#comments"><!-- react-text: 2673 -->40<!-- /react-text --><!-- react-text: 2674 --> 条评论<!-- /react-text --></a></span></div></div></div></li></ul><div class="ColumnEnd"><i class="ColumnEnd-icon icon-ic_column_end"></i></div></div></div><!-- react-empty: 1139 --><!-- react-empty: 1140 --></div></div><!-- react-empty: 126 --><!-- react-empty: 127 --><!-- react-empty: 128 --></div></div>
    <textarea id="clientConfig" hidden="">{"debug":false,"apiRoot":"","paySDK":"https://pay.zhihu.com/api/js","wechatConfigAPI":"/api/wechat/jssdkconfig","name":"production","instance":"column","tokens":{"X-XSRF-TOKEN":"2|2d49daca|4b28b8a84e28e2f2007bbefd4b64eea84870f7ab142fefe7142cbcfa187ee3f84c7dbba9|1502342858","X-UDID":"\"AIDCYRgqCwuPTsX9cVP9sA9biwwBnl70G5I=|1482507793\"","Authorization":["Mi4wQUFDQVlFVWVBQUFBZ01KaEdDb0xDeGNBQUFCaEFsVk5lcnVzV1FCQXRXZ2czbGF4N0lDbU1CYTFjdVVXTW9FMklB","1501900410","ec1050fb528987c11f7505f3b764a75ad843aaee"]}}</textarea>
    <textarea id="preloadedState" hidden="">{"database":{"Post":{"21725498":{"title":"深度增强学习之Policy Gradient方法1","author":"flood-sung","content":"&lt;h2&gt;1 前言&lt;/h2&gt;&lt;p&gt;在之前的深度增强学习系列文章中，我们已经详细分析了DQN算法，一种基于价值Value的算法，那么在今天，我们和大家一起分析深度增强学习中的另一种算法，也就是基于策略梯度Policy Gradient的算法。这种算法和基于价值Value的算法结合而成的Actor-Critic算法是目前效果最好的深度增强学习算法。&lt;/p&gt;&lt;p&gt;那么关于Policy Gradient方法的学习，有以下一些网上的资源值得看：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Andrej Karpathy blog: &lt;a href=\"http://link.zhihu.com/?target=http%3A//karpathy.github.io/2016/05/31/rl/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Deep Reinforcement Learning: Pong from Pixels&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;David Silver ICML 2016：&lt;a href=\"http://link.zhihu.com/?target=http%3A//icml.cc/2016/tutorials/deep_rl_tutorial.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;深度增强学习Tutorial&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;John Schulman：&lt;a href=\"http://link.zhihu.com/?target=http%3A//learning.mpi-sws.org/mlss2016/speakers/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Machine Learning Summer School&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;David Silver的增强学习课程（有视频和ppt）: &lt;a href=\"http://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;&lt;span class=\"invisible\"&gt;http://&lt;/span&gt;&lt;span class=\"visible\"&gt;www0.cs.ucl.ac.uk/staff&lt;/span&gt;&lt;span class=\"invisible\"&gt;/D.Silver/web/Teaching.html&lt;/span&gt;&lt;span class=\"ellipsis\"&gt;&lt;/span&gt;&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;那么实际上Andrej Karpathy的blog已经很详细的分析了Policy Gradient的方法，这里我将综合以上的内容根据我自己的理解来说以下Policy Gradient。&lt;/p&gt;&lt;h2&gt;2 Why Policy Network?&lt;/h2&gt;&lt;p&gt;我们已经知道DQN是一个基于价值value的方法。换句话说就是通过计算每一个状态动作的价值，然后选择价值最大的动作执行。这是一种间接的做法。那么，更直接的做法是什么？&lt;/p&gt;&lt;p&gt;&lt;b&gt;能不能直接更新策略网络Policy Network呢？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;什么是策略网络Policy Network？就是一个神经网络，输入是状态，输出直接就是动作（不是Q值）。&lt;/p&gt;&lt;img src=\"http://www.zhihu.com/equation?tex=a+%3D+%5Cpi%28s%2C%5Ctheta%29\" alt=\"a = \\pi(s,\\theta)\" eeimg=\"1\"&gt;&lt;img src=\"http://www.zhihu.com/equation?tex=a+%3D+%5Cpi%28s%2C%5Ctheta%29++\" alt=\"a = \\pi(s,\\theta)  \" eeimg=\"1\"&gt;&lt;br&gt;&lt;p&gt;或者输出概率：&lt;img src=\"http://www.zhihu.com/equation?tex=a+%3D+%5Cpi%28a%7Cs%2C%5Ctheta%29\" alt=\"a = \\pi(a|s,\\theta)\" eeimg=\"1\"&gt;&lt;/p&gt;&lt;p&gt;这里要提一下概率输出的问题。对于DQN来说，本质上是一个接近于确定性输出的算法。至多就是采用&lt;img src=\"http://www.zhihu.com/equation?tex=%5Cepsilon-greedy\" alt=\"\\epsilon-greedy\" eeimg=\"1\"&gt;进行探索。但是有很多时候，在某一个特定状态下，很多动作的选择可能都是可以的。比如说我有20块钱去买饭。那么不管我买的是蛋炒饭还是土豆肉片盖码饭，结果都是一样的填饱肚子。因此，采用输出概率会更通用一些。而DQN并不能输出动作的概率，所以采用Policy Network是一个更好的办法。&lt;/p&gt;&lt;h2&gt;3 Policy Gradient&lt;/h2&gt;&lt;p&gt;要更新策略网络，或者说要使用梯度下降的方法来更新网络，我们需要有一个目标函数。对于策略网络，目标函数其实是比较容易给定的，就是很直接的，最后的结果！也就是&lt;/p&gt;&lt;p&gt;&lt;img src=\"http://www.zhihu.com/equation?tex=L%28%5Ctheta%29+%3D+%5Cmathbb+E%28r_1%2B%5Cgamma+r_2+%2B+%5Cgamma%5E2+r_3+%2B+...%7C%5Cpi%28%2C%5Ctheta%29%29\" alt=\"L(\\theta) = \\mathbb E(r_1+\\gamma r_2 + \\gamma^2 r_3 + ...|\\pi(,\\theta))\" eeimg=\"1\"&gt; 所有带衰减reward的累加期望&lt;br&gt;&lt;/p&gt;&lt;p&gt;那么问题就在于如何利用这个目标来更新参数&lt;img src=\"http://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"&gt;呢？咋一看这个损失函数和策略网络简直没有什么直接联系，reward是环境给出的，如何才能更新参数？换个说法就是如何能够计算出损失函数关于参数的梯度（也就是策略梯度）：&lt;/p&gt;&lt;img src=\"http://www.zhihu.com/equation?tex=%5Cnabla_%7B%5Ctheta%7D+L%28%5Ctheta%29\" alt=\"\\nabla_{\\theta} L(\\theta)\" eeimg=\"1\"&gt;&lt;p&gt;咋一看根本就没有什么思路是不是，所以先换一个思路来考虑问题。&lt;/p&gt;&lt;h2&gt;4 就给我一个Policy Network，也没有loss，怎么更新？&lt;/h2&gt;&lt;blockquote&gt;改变动作的出现概率！&lt;br&gt;&lt;/blockquote&gt;&lt;p&gt;现在我们不考虑别的，就仅仅从概率的角度来思考问题。我们有一个策略网络，输入状态，输出动作的概率。然后执行完动作之后，我们可以得到reward，或者result。那么这个时候，我们有个非常简单的想法：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;如果某一个动作得到reward多，那么我们就使其出现的概率增大，如果某一个动作得到的reward少，那么我们就使其出现的概率减小。&lt;br&gt;&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;当然，也显然的，用reward来评判动作的好坏是不准确的，甚至用result来评判也是不准确的。毕竟任何一个reward，result都依赖于大量的动作才导致的。但是这并不妨碍我们做这样的思考：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;如果能够构造一个好的动作评判指标，来判断一个动作的好与坏，那么我们就可以通过改变动作的出现概率来优化策略！&lt;br&gt;&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;假设这个评价指标是&lt;img src=\"http://www.zhihu.com/equation?tex=f%28s%2Ca%29\" alt=\"f(s,a)\" eeimg=\"1\"&gt;,那么我们的Policy Network输出的是概率。一般情况下，更常使用log likelihood &lt;img src=\"http://www.zhihu.com/equation?tex=log+%5Cpi%28a%7Cs%2C%5Ctheta%29\" alt=\"log \\pi(a|s,\\theta)\" eeimg=\"1\"&gt;。原因的话看这里&lt;a href=\"http://link.zhihu.com/?target=http%3A//math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Why we consider log likelihood instead of Likelihood in Gaussian Distribution&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;因此，我们就可以构造一个损失函数如下：&lt;/p&gt;&lt;img src=\"http://www.zhihu.com/equation?tex=L%28%5Ctheta%29+%3D+%5Csum+log%5Cpi%28a%7Cs%2C%5Ctheta%29f%28s%2Ca%29\" alt=\"L(\\theta) = \\sum log\\pi(a|s,\\theta)f(s,a)\" eeimg=\"1\"&gt;&lt;br&gt;&lt;p&gt;怎么理解呢？举个简单的AlphaGo的例子吧。对于AlphaGo而言，f(s,a)就是最后的结果。也就是一盘棋中，如果这盘棋赢了，那么这盘棋下的每一步都是认为是好的，如果输了，那么都认为是不好的。好的f(s,a)就是1，不好的就-1。所以在这里，如果a被认为是好的，那么目标就是最大化这个好的动作的概率，反之亦然。&lt;/p&gt;&lt;p&gt;这就是Policy Gradient最基本的思想。&lt;/p&gt;&lt;h2&gt;5 另一个角度：直接算&lt;/h2&gt;&lt;p&gt;f(s,a)不仅仅可以作为动作的评价指标，还可以作为目标函数。就如同AlphaGo，评价指标就是赢或者输，而目标就是结果赢。这和之前分析的目标完全没有冲突。因此，我们可以利用评价指标f(s,a)来优化Policy，同时也是在优化的同时优化了f(s,a).那么问题就变成对f(s,a)求关于参数的梯度。下面的公式直接摘自Andrej Karpathy的blog，f(x)即是f(s,a)&lt;/p&gt;&lt;img src=\"http://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D%0A%5Cnabla_%7B%5Ctheta%7D+E_x%5Bf%28x%29%5D+%26%3D+%5Cnabla_%7B%5Ctheta%7D+%5Csum_x+p%28x%29+f%28x%29+%26+%5Ctext%7Bdefinition+of+expectation%7D+%5C%5C%0A%26+%3D+%5Csum_x+%5Cnabla_%7B%5Ctheta%7D+p%28x%29+f%28x%29+%26+%5Ctext%7Bswap+sum+and+gradient%7D+%5C%5C%0A%26+%3D+%5Csum_x+p%28x%29+%5Cfrac%7B%5Cnabla_%7B%5Ctheta%7D+p%28x%29%7D%7Bp%28x%29%7D+f%28x%29+%26+%5Ctext%7Bboth+multiply+and+divide+by+%7D+p%28x%29+%5C%5C%0A%26+%3D+%5Csum_x+p%28x%29+%5Cnabla_%7B%5Ctheta%7D+%5Clog+p%28x%29+f%28x%29+%26+%5Ctext%7Buse+the+fact+that+%7D+%5Cnabla_%7B%5Ctheta%7D+%5Clog%28z%29+%3D+%5Cfrac%7B1%7D%7Bz%7D+%5Cnabla_%7B%5Ctheta%7D+z+%5C%5C%0A%26+%3D+E_x%5Bf%28x%29+%5Cnabla_%7B%5Ctheta%7D+%5Clog+p%28x%29+%5D+%26+%5Ctext%7Bdefinition+of+expectation%7D%0A%5Cend%7Balign%7D\" alt=\"\\begin{align}\n\\nabla_{\\theta} E_x[f(x)] &amp;amp;= \\nabla_{\\theta} \\sum_x p(x) f(x) &amp;amp; \\text{definition of expectation} \\\\\n&amp;amp; = \\sum_x \\nabla_{\\theta} p(x) f(x) &amp;amp; \\text{swap sum and gradient} \\\\\n&amp;amp; = \\sum_x p(x) \\frac{\\nabla_{\\theta} p(x)}{p(x)} f(x) &amp;amp; \\text{both multiply and divide by } p(x) \\\\\n&amp;amp; = \\sum_x p(x) \\nabla_{\\theta} \\log p(x) f(x) &amp;amp; \\text{use the fact that } \\nabla_{\\theta} \\log(z) = \\frac{1}{z} \\nabla_{\\theta} z \\\\\n&amp;amp; = E_x[f(x) \\nabla_{\\theta} \\log p(x) ] &amp;amp; \\text{definition of expectation}\n\\end{align}\" eeimg=\"1\"&gt;&lt;br&gt;&lt;p&gt;从公式得到的结论可以看到正好和上一小结分析得到的目标函数一致。&lt;/p&gt;&lt;p&gt;因此，Policy Gradient方法就这么确定了。&lt;/p&gt;&lt;h2&gt;6 小结&lt;/h2&gt;&lt;p&gt;本篇blog作为一个引子，介绍下Policy Gradient的基本思想。那么大家会发现，如何确定这个评价指标才是实现Policy Gradient方法的关键所在。所以，在下一篇文章中。我们将来分析一下这个评价指标的问题。&lt;/p&gt;","updated":"2016-08-03T03:04:25.000Z","canComment":true,"commentPermission":"anyone","commentCount":17,"collapsedCount":0,"likeCount":128,"state":"published","isLiked":false,"slug":"21725498","lastestTipjarors":[{"isFollowed":false,"name":"二大爷依然饭特稀","headline":"","avatarUrl":"https://pic3.zhimg.com/400bef53c53177878be2db63e08acdba_s.jpg","isFollowing":false,"type":"people","slug":"gua-niu2","bio":"我是谁...我在哪...我为什么这么帅...","hash":"bf6f7428561f5ed61eddaae6f920c7aa","uid":628500424236339200,"isOrg":false,"description":"","profileUrl":"https://www.zhihu.com/people/gua-niu2","avatar":{"id":"400bef53c53177878be2db63e08acdba","template":"https://pic3.zhimg.com/{id}_{size}.jpg"},"isOrgWhiteList":false},{"isFollowed":false,"name":"ryenC","headline":"","avatarUrl":"https://pic3.zhimg.com/aafbfedee_s.jpg","isFollowing":false,"type":"people","slug":"rryen","bio":null,"hash":"5e15e9f871f2d8646283e0f530032f8d","uid":28157243555840,"isOrg":false,"description":"","profileUrl":"https://www.zhihu.com/people/rryen","avatar":{"id":"aafbfedee","template":"https://pic3.zhimg.com/{id}_{size}.jpg"},"isOrgWhiteList":false},{"isFollowed":false,"name":"CosmosShadow","headline":"","avatarUrl":"https://pic4.zhimg.com/ed24b76e87836967fc750a8ebbb61d8b_s.png","isFollowing":false,"type":"people","slug":"CosmosShadow","bio":null,"hash":"9edbd0a72e71e02efe3ecd3f226ee4bc","uid":31209249832960,"isOrg":false,"description":"","profileUrl":"https://www.zhihu.com/people/CosmosShadow","avatar":{"id":"ed24b76e87836967fc750a8ebbb61d8b","template":"https://pic4.zhimg.com/{id}_{size}.png"},"isOrgWhiteList":false}],"isTitleImageFullScreen":false,"rating":"none","titleImage":"https://pic2.zhimg.com/6f589df38509d14f839737645322a011_r.jpg","links":{"comments":"/api/posts/21725498/comments"},"reviewers":[],"topics":[{"url":"https://www.zhihu.com/topic/19551275","id":"19551275","name":"人工智能"},{"url":"https://www.zhihu.com/topic/20039099","id":"20039099","name":"强化学习 (Reinforcement Learning)"},{"url":"https://www.zhihu.com/topic/19553510","id":"19553510","name":"算法"}],"adminClosedComment":false,"titleImageSize":{"width":1140,"height":440},"href":"/api/posts/21725498","excerptTitle":"","column":{"slug":"intelligentunit","name":"智能单元"},"tipjarState":"activated","tipjarTagLine":"真诚赞赏，手留余香","sourceUrl":"","pageCommentsCount":17,"tipjarorCount":3,"annotationAction":[],"hasPublishingDraft":false,"snapshotUrl":"","publishedTime":"2016-08-03T11:04:25+08:00","url":"/p/21725498","lastestLikers":[{"bio":null,"isFollowing":false,"hash":"ebb3503f26fbe4878cef4fd5f55f786b","uid":878910501244006400,"isOrg":false,"slug":"zane-27-42","isFollowed":false,"description":"","name":"Zane","profileUrl":"https://www.zhihu.com/people/zane-27-42","avatar":{"id":"da8e974dc","template":"https://pic1.zhimg.com/{id}_{size}.jpg"},"isOrgWhiteList":false},{"bio":"遇见未来","isFollowing":false,"hash":"5c9e27cf1fef27ef0200668cebf8d5c6","uid":41714324602880,"isOrg":false,"slug":"MaxKevin","isFollowed":false,"description":"","name":"丁飞","profileUrl":"https://www.zhihu.com/people/MaxKevin","avatar":{"id":"da8e974dc","template":"https://pic1.zhimg.com/{id}_{size}.jpg"},"isOrgWhiteList":false},{"bio":null,"isFollowing":false,"hash":"1474bc7ff2318016d55c1b471f8bffed","uid":30529999077376,"isOrg":false,"slug":"huo-ge-11","isFollowed":false,"description":"=。=","name":"HH AHH","profileUrl":"https://www.zhihu.com/people/huo-ge-11","avatar":{"id":"v2-9598ea25a780eab1401bb35d360b1c0c","template":"https://pic1.zhimg.com/{id}_{size}.jpg"},"isOrgWhiteList":false},{"bio":null,"isFollowing":false,"hash":"fc7709c549ca4355cde73749664ce6da","uid":48790589407232,"isOrg":false,"slug":"ganxiaochu","isFollowed":false,"description":"","name":"甘小楚","profileUrl":"https://www.zhihu.com/people/ganxiaochu","avatar":{"id":"v2-3dfead3959325c421054419aa9068fc0","template":"https://pic1.zhimg.com/{id}_{size}.jpg"},"isOrgWhiteList":false},{"bio":null,"isFollowing":false,"hash":"2939133b7b358bfd0fc0a88ddcac0f55","uid":646428868278358000,"isOrg":false,"slug":"coeus-30","isFollowed":false,"description":"","name":"coeus","profileUrl":"https://www.zhihu.com/people/coeus-30","avatar":{"id":"da8e974dc","template":"https://pic1.zhimg.com/{id}_{size}.jpg"},"isOrgWhiteList":false}],"summary":"1 前言在之前的深度增强学习系列文章中，我们已经详细分析了DQN算法，一种基于价值Value的算法，那么在今天，我们和大家一起分析深度增强学习中的另一种算法，也就是基于策略梯度Policy Gradient的算法。这种算法和基于价值Value的算法结合而成的Actor-Crit…","reviewingCommentsCount":0,"meta":{"previous":{"isTitleImageFullScreen":false,"rating":"none","titleImage":"https://pic3.zhimg.com/b629f243297cf32d2507bdaa1bc38e12_r.jpg","links":{"comments":"/api/posts/21741716/comments"},"topics":[{"url":"https://www.zhihu.com/topic/19559450","id":"19559450","name":"机器学习"},{"url":"https://www.zhihu.com/topic/19813032","id":"19813032","name":"深度学习（Deep Learning）"},{"url":"https://www.zhihu.com/topic/19551275","id":"19551275","name":"人工智能"}],"adminClosedComment":false,"href":"/api/posts/21741716","excerptTitle":"","author":{"bio":"CS231n资源在专栏文章中","isFollowing":false,"hash":"928affb05b0b70a2c12e109d63b6bae5","uid":27591822016512,"isOrg":false,"slug":"du-ke","isFollowed":false,"description":"研究增强学习。积累学习方法论，实践健身训练体系。专栏：https://zhuanlan.zhihu.com/intelligentunit","name":"杜客","profileUrl":"https://www.zhihu.com/people/du-ke","avatar":{"id":"5ab5b93bd","template":"https://pic2.zhimg.com/{id}_{size}.jpg"},"isOrgWhiteList":false},"column":{"slug":"intelligentunit","name":"智能单元"},"content":"译者注：本文&lt;a href=\"https://zhuanlan.zhihu.com/intelligentunit\" class=\"internal\"&gt;智能单元&lt;/a&gt;首发，译自斯坦福CS231n课程笔记&lt;a href=\"http://link.zhihu.com/?target=http%3A//cs231n.github.io/neural-networks-3/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Neural Nets notes 3&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;，课程教师&lt;a href=\"http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Andrej Karpathy&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;授权翻译。本篇教程由&lt;a href=\"https://www.zhihu.com/people/du-ke\" class=\"internal\"&gt;杜客&lt;/a&gt;翻译完成，&lt;a href=\"https://www.zhihu.com/people/kun-kun-97-81\" class=\"internal\"&gt;堃堃&lt;/a&gt;和&lt;a href=\"https://www.zhihu.com/people/hmonkey\" class=\"internal\"&gt;巩子嘉&lt;/a&gt;进行校对修改。译文含公式和代码，建议PC端阅读。&lt;h2&gt;原文如下&lt;/h2&gt;&lt;p&gt;内容列表：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;梯度检查&lt;/li&gt;&lt;li&gt;合理性（Sanity）检查&lt;/li&gt;&lt;li&gt;检查学习过程&lt;ul&gt;&lt;li&gt;损失函数&lt;/li&gt;&lt;li&gt;训练集与验证集准确率&lt;/li&gt;&lt;li&gt;权重：更新比例&lt;/li&gt;&lt;li&gt;每层的激活数据与梯度分布&lt;/li&gt;&lt;li&gt;可视化 &lt;b&gt;&lt;i&gt;译者注：上篇翻译截止处&lt;/i&gt;&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;参数更新&lt;ul&gt;&lt;li&gt;一阶（随机梯度下降）方法，动量方法，Nesterov动量方法&lt;/li&gt;&lt;li&gt;学习率退火&lt;/li&gt;&lt;li&gt;二阶方法&lt;/li&gt;&lt;li&gt;逐参数适应学习率方法（Adagrad，RMSProp）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;超参数调优&lt;/li&gt;&lt;li&gt;评价&lt;ul&gt;&lt;li&gt;模型集成&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;总结&lt;/li&gt;&lt;li&gt;拓展引用&lt;/li&gt;&lt;/ul&gt;&lt;h1&gt;学习过程&lt;/h1&gt;&lt;p&gt;在前面章节中，我们讨论了神经网络的静态部分：如何创建网络的连接、数据和损失函数。本节将致力于讲解神经网络的动态部分，即神经网络学习参数和搜索最优超参数的过程。&lt;br&gt;&lt;/p&gt;&lt;h2&gt;梯度检查&lt;/h2&gt;&lt;p&gt;理论上将进行梯度检查很简单，就是简单地把解析梯度和数值计算梯度进行比较。然而从实际操作层面上来说，这个过程更加复杂且容易出错。下面是一些提示、技巧和需要仔细注意的事情：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;使用中心化公式。&lt;/strong&gt;在使用有限差值近似来计算数值梯度的时候，常见的公式是：&lt;/p&gt;&lt;img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cfrac%7Bdf%28x%29%7D%7Bdx%7D%3D%5Cfrac%7Bf%28x%2Bh%29-f%28x%29%7D%7Bh%7D%28bad%2C%5C+do%5C+not%5C+use%29\" alt=\"\\displaystyle \\frac{df(x)}{dx}=\\frac{f(x+h)-f(x)}{h}(bad,\\ do\\ not\\ use)\" eeimg=\"1\"&gt;&lt;br&gt;&lt;p&gt;其中&lt;img src=\"http://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\"&gt;是一个很小的数字，在实践中近似为1e-5。在实践中证明，使用&lt;i&gt;中心化&lt;/i&gt;公式效果更好：&lt;br&gt;&lt;/p&gt;&lt;img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cfrac%7Bdf%28x%29%7D%7Bdx%7D%3D%5Cfrac%7Bf%28x%2Bh%29-f%28x-h%29%7D%7B2h%7D%28use%5C+instead%29\" alt=\"\\displaystyle \\frac{df(x)}{dx}=\\frac{f(x+h)-f(x-h)}{2h}(use\\ instead)\" eeimg=\"1\"&gt;&lt;br&gt;&lt;p&gt;该公式在检查梯度的每个维度的时候，会要求计算两次损失函数（所以计算资源的耗费也是两倍），但是梯度的近似值会准确很多。要理解这一点，对&lt;img src=\"http://www.zhihu.com/equation?tex=f%28x%2Bh%29\" alt=\"f(x+h)\" eeimg=\"1\"&gt;和&lt;img src=\"http://www.zhihu.com/equation?tex=f%28x-h%29\" alt=\"f(x-h)\" eeimg=\"1\"&gt;使用泰勒展开，可以看到第一个公式的误差近似&lt;img src=\"http://www.zhihu.com/equation?tex=O%28h%29\" alt=\"O(h)\" eeimg=\"1\"&gt;，第二个公式的误差近似&lt;img src=\"http://www.zhihu.com/equation?tex=O%28h%5E2%29\" alt=\"O(h^2)\" eeimg=\"1\"&gt;（是个二阶近似）。&lt;i&gt;&lt;b&gt;（译者注：泰勒展开相关内容可阅读《高等数学》第十二章第四节：函数展开成幂级数。）&lt;/b&gt;&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;使用相对误差来比较&lt;/b&gt;。比较数值梯度&lt;img src=\"http://www.zhihu.com/equation?tex=f%27_n\" alt=\"f'_n\" eeimg=\"1\"&gt;和解析梯度&lt;img src=\"http://www.zhihu.com/equation?tex=f%27_a\" alt=\"f'_a\" eeimg=\"1\"&gt;的细节有哪些？如何得知此两者不匹配？你可能会倾向于监测它们的差的绝对值&lt;img src=\"http://www.zhihu.com/equation?tex=%7Cf%27_a-f%27_n%7C\" alt=\"|f'_a-f'_n|\" eeimg=\"1\"&gt;或者差的平方值，然后定义该值如果超过某个规定阈值，就判断梯度实现失败。然而该思路是有问题的。想想，假设这个差值是1e-4，如果两个梯度值在1.0左右，这个差值看起来就很合适，可以认为两个梯度是匹配的。然而如果梯度值是1e-5或者更低，那么1e-4就是非常大的差距，梯度实现肯定就是失败的了。因此，使用&lt;i&gt;相对误差&lt;/i&gt;总是更合适一些：&lt;br&gt;&lt;/p&gt;&lt;img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle+%5Cfrac%7B%7Cf%27_a-f%27_n%7C%7D%7Bmax%28%7Cf%27_a%7C%2C%7Cf%27_n%7C%29%7D\" alt=\"\\displaystyle \\frac{|f'_a-f'_n|}{max(|f'_a|,|f'_n|)}\" eeimg=\"1\"&gt;&lt;br&gt;&lt;p&gt;上式考虑了差值占两个梯度绝对值的比例。注意通常相对误差公式只包含两个式子中的一个（任意一个均可），但是我更倾向取两个式子的最大值或者取两个式子的和。这样做是为了防止在其中一个式子为0时，公式分母为0（这种情况，在ReLU中是经常发生的）。然而，还必须注意两个式子都为零且通过梯度检查的情况。在实践中：&lt;br&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;相对误差&amp;gt;1e-2：通常就意味着梯度可能出错。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;1e-2&amp;gt;相对误差&amp;gt;1e-4：要对这个值感到不舒服才行。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;1e-4&amp;gt;相对误差：这个值的相对误差对于有不可导点的目标函数是OK的。但如果目标函数中没有kink（使用tanh和softmax），那么相对误差值还是太高。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;1e-7或者更小：好结果，可以高兴一把了。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;要知道的是网络的深度越深，相对误差就越高。所以如果你是在对一个10层网络的输入数据做梯度检查，那么1e-2的相对误差值可能就OK了，因为误差一直在累积。相反，如果一个可微函数的相对误差值是1e-2，那么通常说明梯度实现不正确。&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;strong&gt;使用双精度。&lt;/strong&gt;一个常见的错误是使用单精度浮点数来进行梯度检查。这样会导致即使梯度实现正确，相对误差值也会很高（比如1e-2）。在我的经验而言，出现过使用单精度浮点数时相对误差为1e-2，换成双精度浮点数时就降低为1e-8的情况。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;保持在浮点数的有效范围。&lt;/strong&gt;建议通读《&lt;a href=\"http://link.zhihu.com/?target=http%3A//docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;What Every Computer Scientist Should Konw About Floating-Point Artthmetic&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;》一文，该文将阐明你可能犯的错误，促使你写下更加细心的代码。例如，在神经网络中，在一个批量的数据上对损失函数进行归一化是很常见的。但是，如果每个数据点的梯度很小，然后又用数据点的数量去除，就使得数值更小，这反过来会导致更多的数值问题。这就是我为什么总是会把原始的解析梯度和数值梯度数据打印出来，确保用来比较的数字的值不是过小（通常绝对值小于1e-10就绝对让人担心）。如果确实过小，可以使用一个常数暂时将损失函数的数值范围扩展到一个更“好”的范围，在这个范围中浮点数变得更加致密。比较理想的是1.0的数量级上，即当浮点数指数为0时。&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;b&gt;目标函数的不可导点（kinks）&lt;/b&gt;。在进行梯度检查时，一个导致不准确的原因是不可导点问题。不可导点是指目标函数不可导的部分，由ReLU（&lt;img src=\"http://www.zhihu.com/equation?tex=max%280%2Cx%29\" alt=\"max(0,x)\" eeimg=\"1\"&gt;）等函数，或SVM损失，Maxout神经元等引入。考虑当&lt;img src=\"http://www.zhihu.com/equation?tex=x%3D-1e6\" alt=\"x=-1e6\" eeimg=\"1\"&gt;的时，对ReLU函数进行梯度检查。因为&lt;img src=\"http://www.zhihu.com/equation?tex=x%3C0\" alt=\"x&amp;lt;0\" eeimg=\"1\"&gt;，所以解析梯度在该点的梯度为0。然而，在这里数值梯度会突然计算出一个非零的梯度值，因为&lt;img src=\"http://www.zhihu.com/equation?tex=f%28x%2Bh%29\" alt=\"f(x+h)\" eeimg=\"1\"&gt;可能越过了不可导点(例如：如果&lt;img src=\"http://www.zhihu.com/equation?tex=h%3E1e-6\" alt=\"h&amp;gt;1e-6\" eeimg=\"1\"&gt;)，导致了一个非零的结果。你可能会认为这是一个极端的案例，但实际上这种情况很常见。例如，一个用CIFAR-10训练的SVM中，因为有50,000个样本，且根据目标函数每个样本产生9个式子，所以包含有450,000个&lt;img src=\"http://www.zhihu.com/equation?tex=max%280%2Cx%29\" alt=\"max(0,x)\" eeimg=\"1\"&gt;式子。而一个用SVM进行分类的神经网络因为采用了ReLU，还会有更多的不可导点。&lt;/p&gt;&lt;br&gt;&lt;p&gt;注意，在计算损失的过程中是可以知道不可导点有没有被越过的。在具有&lt;img src=\"http://www.zhihu.com/equation?tex=max%28x%2Cy%29\" alt=\"max(x,y)\" eeimg=\"1\"&gt;形式的函数中持续跟踪所有“赢家”的身份，就可以实现这一点。其实就是看在前向传播时，到底x和y谁更大。如果在计算&lt;img src=\"http://www.zhihu.com/equation?tex=f%28x%2Bh%29\" alt=\"f(x+h)\" eeimg=\"1\"&gt;和&lt;img src=\"http://www.zhihu.com/equation?tex=f%28x-h%29\" alt=\"f(x-h)\" eeimg=\"1\"&gt;的时候，至少有一个“赢家”的身份变了，那就说明不可导点被越过了，数值梯度会不准确。&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;strong&gt;使用少量数据点。&lt;/strong&gt;解决上面的不可导点问题的一个办法是使用更少的数据点。因为含有不可导点的损失函数(例如：因为使用了ReLU或者边缘损失等函数)的数据点越少，不可导点就越少，所以在计算有限差值近似时越过不可导点的几率就越小。还有，如果你的梯度检查对2-3个数据点都有效，那么基本上对整个批量数据进行梯度检查也是没问题的。所以使用很少量的数据点，能让梯度检查更迅速高效。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;谨慎设置步长h。&lt;/strong&gt;在实践中h并不是越小越好，因为当&lt;img src=\"http://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\"&gt;特别小的时候，就可能就会遇到数值精度问题。有时候如果梯度检查无法进行，可以试试将&lt;img src=\"http://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\"&gt;调到1e-4或者1e-6，然后突然梯度检查可能就恢复正常。这篇&lt;a href=\"http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Numerical_differentiation\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;维基百科文章&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;中有一个图表，其x轴为&lt;img src=\"http://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\"&gt;值，y轴为数值梯度误差。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;在操作的特性模式中梯度检查。&lt;/strong&gt;有一点必须要认识到：梯度检查是在参数空间中的一个特定（往往还是随机的）的单独点进行的。即使是在该点上梯度检查成功了，也不能马上确保全局上梯度的实现都是正确的。还有，一个随机的初始化可能不是参数空间最优代表性的点，这可能导致进入某种病态的情况，即梯度看起来是正确实现了，实际上并没有。例如，SVM使用小数值权重初始化，就会把一些接近于0的得分分配给所有的数据点，而梯度将会在所有的数据点上展现出某种模式。一个不正确实现的梯度也许依然能够产生出这种模式，但是不能泛化到更具代表性的操作模式，比如在一些的得分比另一些得分更大的情况下就不行。因此为了安全起见，最好让网络学习（“预热”）一小段时间，等到损失函数开始下降的之后再进行梯度检查。在第一次迭代就进行梯度检查的危险就在于，此时可能正处在不正常的边界情况，从而掩盖了梯度没有正确实现的事实。&lt;/p&gt;&lt;br&gt;&lt;p&gt;&lt;strong&gt;不要让正则化吞没数据。&lt;/strong&gt;通常损失函数是数据损失和正则化损失的和（例如L2对权重的惩罚）。需要注意的危险是正则化损失可能吞没掉数据损失，在这种情况下梯度主要来源于正则化部分（正则化部分的梯度表达式通常简单很多）。这样就会掩盖掉数据损失梯度的不正确实现。因此，推荐先关掉正则化对数据损失做单独检查，然后对正则化做单独检查。对于正则化的单独检查可以是修改代码，去掉其中数据损失的部分，也可以提高正则化强度，确认其效果在梯度检查中是无法忽略的，这样不正确的实现就会被观察到了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;记得关闭随机失活（dropout）和数据扩张（augmentation）&lt;/b&gt;。在进行梯度检查时，记得关闭网络中任何不确定的效果的操作，比如随机失活，随机数据扩展等。不然它们会在计算数值梯度的时候导致巨大误差。关闭这些操作不好的一点是无法对它们进行梯度检查（例如随机失活的反向传播实现可能有错误）。因此，一个更好的解决方案就是在计算&lt;img src=\"http://www.zhihu.com/equation?tex=f%28x%2Bh%29\" alt=\"f(x+h)\" eeimg=\"1\"&gt;和&lt;img src=\"http://www.zhihu.com/equation?tex=f%28x-h%29\" alt=\"f(x-h)\" eeimg=\"1\"&gt;前强制增加一个特定的随机种子，在计算解析梯度时也同样如此。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;检查少量的维度。&lt;/strong&gt;在实际中，梯度可以有上百万的参数，在这种情况下只能检查其中一些维度然后假设其他维度是正确的。&lt;strong&gt;注意&lt;/strong&gt;&lt;b&gt;：&lt;/b&gt;确认在所有不同的参数中都抽取一部分来梯度检查。在某些应用中，为了方便，人们将所有的参数放到一个巨大的参数向量中。在这种情况下，例如偏置就可能只占用整个向量中的很小一部分，所以不要随机地从向量中取维度，一定要把这种情况考虑到，确保所有参数都收到了正确的梯度。&lt;br&gt;&lt;/p&gt;&lt;br&gt;&lt;h2&gt;学习之前：合理性检查的提示与技巧&lt;/h2&gt;&lt;p&gt;在进行费时费力的最优化之前，最好进行一些合理性检查：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;寻找特定情况的正确损失值。&lt;/strong&gt;在使用小参数进行初始化时，确保得到的损失值与期望一致。最好先单独检查数据损失（让正则化强度为0）。例如，对于一个跑CIFAR-10的Softmax分类器，一般期望它的初始损失值是2.302，这是因为初始时预计每个类别的概率是0.1（因为有10个类别），然后Softmax损失值正确分类的负对数概率：-ln(0.1)=2.302。对于Weston Watkins SVM，假设所有的边界都被越过（因为所有的分值都近似为零），所以损失值是9（因为对于每个错误分类，边界值是1）。如果没看到这些损失值，那么初始化中就可能有问题。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;第二个合理性检查：提高正则化强度时导致损失值变大。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;对小数据子集过拟合。&lt;/strong&gt;最后也是最重要的一步，在整个数据集进行训练之前，尝试在一个很小的数据集上进行训练（比如20个数据），然后确保能到达0的损失值。进行这个实验的时候，最好让正则化强度为0，不然它会阻止得到0的损失。除非能通过这一个正常性检查，不然进行整个数据集训练是没有意义的。但是注意，能对小数据集进行过拟合并不代表万事大吉，依然有可能存在不正确的实现。比如，因为某些错误，数据点的特征是随机的，这样算法也可能对小数据进行过拟合，但是在整个数据集上跑算法的时候，就没有任何泛化能力。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;h2&gt;检查整个学习过程&lt;/h2&gt;&lt;p&gt;在训练神经网络的时候，应该跟踪多个重要数值。这些数值输出的图表是观察训练进程的一扇窗口，是直观理解不同的超参数设置效果的工具，从而知道如何修改超参数以获得更高效的学习过程。&lt;/p&gt;&lt;br&gt;&lt;p&gt;在下面的图表中，x轴通常都是表示&lt;b&gt;&lt;u&gt;周期（epochs）&lt;/u&gt;&lt;/b&gt;单位，该单位衡量了在训练中每个样本数据都被观察过次数的期望（一个周期意味着每个样本数据都被观察过了一次）。相较于迭代次数（iterations），一般更倾向跟踪周期，这是因为迭代次数与数据的批尺寸（batchsize）有关，而批尺寸的设置又可以是任意的。&lt;/p&gt;&lt;h2&gt;损失函数&lt;/h2&gt;&lt;p&gt;训练期间第一个要跟踪的数值就是损失值，它在前向传播时对每个独立的批数据进行计算。下图展示的是随着损失值随时间的变化，尤其是曲线形状会给出关于学习率设置的情况：&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img src=\"https://pic2.zhimg.com/753f398b46cc28c1916d6703cf2080f5_b.png\" data-rawwidth=\"1578\" data-rawheight=\"706\" class=\"origin_image zh-lightbox-thumb\" width=\"1578\" data-original=\"https://pic2.zhimg.com/753f398b46cc28c1916d6703cf2080f5_r.png\"&gt;&lt;b&gt;左图&lt;/b&gt;展示了不同的学习率的效果。过低的学习率导致算法的改善是线性的。高一些的学习率会看起来呈几何指数下降，更高的学习率会让损失值很快下降，但是接着就停在一个不好的损失值上（绿线）。这是因为最优化的“能量”太大，参数在混沌中随机震荡，不能最优化到一个很好的点上。&lt;b&gt;右图&lt;/b&gt;显示了一个典型的随时间变化的损失函数值，在CIFAR-10数据集上面训练了一个小的网络，这个损失函数值曲线看起来比较合理（虽然可能学习率有点小，但是很难说），而且指出了批数据的数量可能有点太小（因为损失值的噪音很大）。&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时震荡就会最小，因为每个梯度更新都是单调地优化损失函数（除非学习率设置得过高）。&lt;/p&gt;&lt;p&gt;有的研究者喜欢用对数域对损失函数值作图。因为学习过程一般都是采用指数型的形状，图表就会看起来更像是能够直观理解的直线，而不是呈曲棍球一样的曲线状。还有，如果多个交叉验证模型在一个图上同时输出图像，它们之间的差异就会比较明显。&lt;/p&gt;&lt;p&gt;有时候损失函数看起来很有意思：&lt;a href=\"http://link.zhihu.com/?target=http%3A//lossfunctions.tumblr.com\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;lossfunctions.tumblr.com&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;。&lt;/p&gt;&lt;h3&gt;训练集和验证集准确率&lt;/h3&gt;&lt;p&gt;在训练分类器的时候，需要跟踪的第二重要的数值是验证集和训练集的准确率。这个图表能够展现知道模型过拟合的程度：&lt;br&gt;&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img src=\"https://pic3.zhimg.com/05a6960a01c0204ced8d875ac3d91fba_b.jpg\" data-rawwidth=\"660\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb\" width=\"660\" data-original=\"https://pic3.zhimg.com/05a6960a01c0204ced8d875ac3d91fba_r.jpg\"&gt;在训练集准确率和验证集准确率中间的空隙指明了模型过拟合的程度。在图中，蓝色的验证集曲线显示相较于训练集，验证集的准确率低了很多，这就说明模型有很强的过拟合。遇到这种情况，就应该增大正则化强度（更强的L2权重惩罚，更多的随机失活等）或收集更多的数据。另一种可能就是验证集曲线和训练集曲线如影随形，这种情况说明你的模型容量还不够大：应该通过增加参数数量让模型容量更大些。&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;h3&gt;权重更新比例&lt;/h3&gt;&lt;p&gt;最后一个应该跟踪的量是权重中更新值的数量和全部值的数量之间的比例。注意：是&lt;em&gt;更新的&lt;/em&gt;，而不是原始梯度（比如，在普通sgd中就是梯度乘以学习率）。需要对每个参数集的更新比例进行单独的计算和跟踪。一个经验性的结论是这个比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。下面是具体例子：&lt;/p&gt;&lt;div class=\"highlight\"&gt;&lt;pre&gt;&lt;code class=\"language-python\"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=\"c1\"&gt;# 假设参数向量为W，其梯度向量为dW&lt;/span&gt;\n&lt;span class=\"n\"&gt;param_scale&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt; &lt;span class=\"n\"&gt;np&lt;/span&gt;&lt;span class=\"o\"&gt;.&lt;/span&gt;&lt;span class=\"n\"&gt;linalg&lt;/span&gt;&lt;span class=\"o\"&gt;.&lt;/span&gt;&lt;span class=\"n\"&gt;norm&lt;/span&gt;&lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"n\"&gt;W&lt;/span&gt;&lt;span class=\"o\"&gt;.&lt;/span&gt;&lt;span class=\"n\"&gt;ravel&lt;/span&gt;&lt;span class=\"p\"&gt;())&lt;/span&gt;\n&lt;span class=\"n\"&gt;update&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt; &lt;span class=\"o\"&gt;-&lt;/span&gt;&lt;span class=\"n\"&gt;learning_rate&lt;/span&gt;&lt;span class=\"o\"&gt;*&lt;/span&gt;&lt;span class=\"n\"&gt;dW&lt;/span&gt; &lt;span class=\"c1\"&gt;# 简单SGD更新&lt;/span&gt;\n&lt;span class=\"n\"&gt;update_scale&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt; &lt;span class=\"n\"&gt;np&lt;/span&gt;&lt;span class=\"o\"&gt;.&lt;/span&gt;&lt;span class=\"n\"&gt;linalg&lt;/span&gt;&lt;span class=\"o\"&gt;.&lt;/span&gt;&lt;span class=\"n\"&gt;norm&lt;/span&gt;&lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"n\"&gt;update&lt;/span&gt;&lt;span class=\"o\"&gt;.&lt;/span&gt;&lt;span class=\"n\"&gt;ravel&lt;/span&gt;&lt;span class=\"p\"&gt;())&lt;/span&gt;\n&lt;span class=\"n\"&gt;W&lt;/span&gt; &lt;span class=\"o\"&gt;+=&lt;/span&gt; &lt;span class=\"n\"&gt;update&lt;/span&gt; &lt;span class=\"c1\"&gt;# 实际更新&lt;/span&gt;\n&lt;span class=\"k\"&gt;print&lt;/span&gt; &lt;span class=\"n\"&gt;update_scale&lt;/span&gt; &lt;span class=\"o\"&gt;/&lt;/span&gt; &lt;span class=\"n\"&gt;param_scale&lt;/span&gt; &lt;span class=\"c1\"&gt;# 要得到1e-3左右&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;相较于跟踪最大和最小值，有研究者更喜欢计算和跟踪梯度的范式及其更新。这些矩阵通常是相关的，也能得到近似的结果。&lt;/p&gt;&lt;br&gt;&lt;h3&gt;每层的激活数据及梯度分布&lt;/h3&gt;&lt;p&gt;一个不正确的初始化可能让学习过程变慢，甚至彻底停止。还好，这个问题可以比较简单地诊断出来。其中一个方法是输出网络中所有层的激活数据和梯度分布的柱状图。直观地说，就是如果看到任何奇怪的分布情况，那都不是好兆头。比如，对于使用tanh的神经元，我们应该看到激活数据的值在整个[-1,1]区间中都有分布。如果看到神经元的输出全部是0，或者全都饱和了往-1和1上跑，那肯定就是有问题了。&lt;/p&gt;&lt;h2&gt;第一层可视化&lt;/h2&gt;&lt;p&gt;最后，如果数据是图像像素数据，那么把第一层特征可视化会有帮助：&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img src=\"https://pic3.zhimg.com/96573094f9d7f4b3b188069726840a2e_b.png\" data-rawwidth=\"1602\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb\" width=\"1602\" data-original=\"https://pic3.zhimg.com/96573094f9d7f4b3b188069726840a2e_r.png\"&gt;将神经网络第一层的权重可视化的例子。&lt;b&gt;左图&lt;/b&gt;中的特征充满了噪音，这暗示了网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低。&lt;b&gt;右图&lt;/b&gt;的特征不错，平滑，干净而且种类繁多，说明训练过程进行良好。&lt;/p&gt;&lt;br&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;b&gt;神经网络笔记3 （上）结束。&lt;/b&gt;&lt;br&gt;&lt;/p&gt;&lt;h2&gt;译者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;转载须全文转载且注明原文链接&lt;/b&gt;，否则保留维权权利；&lt;/li&gt;&lt;li&gt;请知友们通过评论和私信等方式批评指正，贡献者均会补充提及；&lt;/li&gt;&lt;li&gt;CS231n的翻译即将进入尾声，&lt;b&gt;欢迎知友们建议后续的翻译方向；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;知友@&lt;a href=\"https://www.zhihu.com/people/ksma\" class=\"internal\"&gt;猪皮&lt;/a&gt;建议下一步的翻译方向是领域内的一些经典论文；&lt;/li&gt;&lt;li&gt;知友@&lt;a href=\"https://www.zhihu.com/people/nan-tian-qi-6\" class=\"internal\"&gt;一蓑烟灰&lt;/a&gt;在评论中详细解释了自己学习CS231n及本科毕设相关情况，建议下一步的翻译方向是课程作业解析。&lt;/li&gt;&lt;/ol&gt;","state":"published","sourceUrl":"","pageCommentsCount":0,"canComment":true,"snapshotUrl":"","slug":21741716,"publishedTime":"2016-08-02T14:26:25+08:00","url":"/p/21741716","title":"CS231n课程笔记翻译：神经网络笔记3（上）","summary":"译者注：本文&lt;a href=\"https://zhuanlan.zhihu.com/intelligentunit\" data-editable=\"true\" data-title=\"智能单元\" class=\"\"&gt;智能单元&lt;/a&gt;首发，译自斯坦福CS231n课程笔记&lt;a href=\"http://cs231n.github.io/neural-networks-3/\" class=\"\" data-editable=\"true\" data-title=\"Neural Nets notes 3\"&gt;Neural Nets notes 3&lt;/a&gt;，课程教师&lt;a href=\"https://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/\" class=\"\" data-editable=\"true\" data-title=\"Andrej Karpathy\"&gt;Andrej Karpathy&lt;/a&gt;授权翻译。本篇教程由&lt;a href=\"https://www.zhihu.com/people/du-ke\" class=\"\" data-editable=\"true\" data-title=\"杜客\"&gt;杜客&lt;/a&gt;翻译完成，&lt;a href=\"https://www.zhihu.com/people/kun-kun-97-81\" class=\"\" data-editable=\"true\" data-title=\"堃堃\"&gt;堃堃&lt;/a&gt;和&lt;a href=\"https://www.zhihu.com/people/hmonkey\" class=\"\" data-editable=\"true\" data-title=\"巩子嘉\"&gt;巩子嘉&lt;/a&gt;进行校对修改。译文含公式和代码，建议PC端阅读。原文如下内容列表：梯度检查合理性（Sanity）检查检查学…","reviewingCommentsCount":0,"meta":{"previous":null,"next":null},"commentPermission":"anyone","commentsCount":42,"likesCount":120},"next":{"isTitleImageFullScreen":false,"rating":"none","titleImage":"https://pic4.zhimg.com/940c2e8d2edc3018771c752d977a6f27_r.png","links":{"comments":"/api/posts/21798784/comments"},"topics":[{"url":"https://www.zhihu.com/topic/19559450","id":"19559450","name":"机器学习"},{"url":"https://www.zhihu.com/topic/19551275","id":"19551275","name":"人工智能"},{"url":"https://www.zhihu.com/topic/19607065","id":"19607065","name":"神经网络"}],"adminClosedComment":false,"href":"/api/posts/21798784","excerptTitle":"","author":{"bio":"CS231n资源在专栏文章中","isFollowing":false,"hash":"928affb05b0b70a2c12e109d63b6bae5","uid":27591822016512,"isOrg":false,"slug":"du-ke","isFollowed":false,"description":"研究增强学习。积累学习方法论，实践健身训练体系。专栏：https://zhuanlan.zhihu.com/intelligentunit","name":"杜客","profileUrl":"https://www.zhihu.com/people/du-ke","avatar":{"id":"5ab5b93bd","template":"https://pic2.zhimg.com/{id}_{size}.jpg"},"isOrgWhiteList":false},"column":{"slug":"intelligentunit","name":"智能单元"},"content":"译者注：本文&lt;a href=\"https://zhuanlan.zhihu.com/intelligentunit\" class=\"internal\"&gt;智能单元&lt;/a&gt;首发，译自斯坦福CS231n课程笔记&lt;a href=\"http://link.zhihu.com/?target=http%3A//cs231n.github.io/neural-networks-3/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Neural Nets notes 3&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;，课程教师&lt;a href=\"http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Andrej Karpathy&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;授权翻译。本篇教程由&lt;a href=\"https://www.zhihu.com/people/du-ke\" class=\"internal\"&gt;杜客&lt;/a&gt;翻译完成，&lt;a href=\"https://www.zhihu.com/people/kun-kun-97-81\" class=\"internal\"&gt;堃堃&lt;/a&gt;和&lt;a href=\"https://www.zhihu.com/people/hmonkey\" class=\"internal\"&gt;巩子嘉&lt;/a&gt;进行校对修改。译文含公式和代码，建议PC端阅读。&lt;h2&gt;原文如下&lt;/h2&gt;&lt;p&gt;内容列表：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;梯度检查&lt;/li&gt;&lt;li&gt;合理性（Sanity）检查&lt;/li&gt;&lt;li&gt;检查学习过程&lt;ul&gt;&lt;li&gt;损失函数&lt;/li&gt;&lt;li&gt;训练与验证准确率&lt;/li&gt;&lt;li&gt;权重：更新比例&lt;/li&gt;&lt;li&gt;每层的激活数据与梯度分布&lt;/li&gt;&lt;li&gt;可视化 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;参数更新 &lt;i&gt;&lt;b&gt;译者注：下篇翻译起始处&lt;/b&gt;&lt;/i&gt;&lt;ul&gt;&lt;li&gt;一阶（随机梯度下降）方法，动量方法，Nesterov动量方法&lt;/li&gt;&lt;li&gt;学习率退火&lt;/li&gt;&lt;li&gt;二阶方法&lt;/li&gt;&lt;li&gt;逐参数适应学习率方法（Adagrad，RMSProp）&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;超参数调优&lt;/li&gt;&lt;li&gt;评价&lt;ul&gt;&lt;li&gt;模型集成&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;总结&lt;/li&gt;&lt;li&gt;拓展引用&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;参数更新&lt;/h2&gt;&lt;p&gt;一旦能使用反向传播计算解析梯度，梯度就能被用来进行参数更新了。进行参数更新有好几种方法，接下来都会进行讨论。&lt;/p&gt;&lt;p&gt;深度网络的最优化是现在非常活跃的研究领域。本节将重点介绍一些公认有效的常用的技巧，这些技巧都是在实践中会遇到的。我们将简要介绍这些技巧的直观概念，但不进行细节分析。对于细节感兴趣的读者，我们提供了一些拓展阅读。&lt;/p&gt;&lt;h3&gt;随机梯度下降及各种更新方法&lt;/h3&gt;&lt;p&gt;&lt;b&gt;普通更新&lt;/b&gt;。最简单的更新形式是沿着负梯度方向改变参数（因为梯度指向的是上升方向，但是我们通常希望最小化损失函数）。假设有一个参数向量&lt;b&gt;x&lt;/b&gt;及其梯度&lt;b&gt;dx&lt;/b&gt;，那么最简单的更新的形式是：&lt;/p&gt;&lt;div class=\"highlight\"&gt;&lt;pre&gt;&lt;code class=\"language-python\"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=\"c1\"&gt;# 普通更新&lt;/span&gt;\n&lt;span class=\"n\"&gt;x&lt;/span&gt; &lt;span class=\"o\"&gt;+=&lt;/span&gt; &lt;span class=\"o\"&gt;-&lt;/span&gt; &lt;span class=\"n\"&gt;learning_rate&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;dx&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中learning_rate是一个超参数，它是一个固定的常量。当在整个数据集上进行计算时，只要学习率足够低，总是能在损失函数上得到非负的进展。&lt;br&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;动量（&lt;/b&gt;&lt;strong&gt;Momentum&lt;/strong&gt;&lt;b&gt;）更新&lt;/b&gt;是另一个方法，这个方法在深度网络上几乎总能得到更好的收敛速度。该方法可以看成是从物理角度上对于最优化问题得到的启发。损失值可以理解为是山的高度（因此高度势能是&lt;img src=\"http://www.zhihu.com/equation?tex=U%3Dmgh\" alt=\"U=mgh\" eeimg=\"1\"&gt;，所以有&lt;img src=\"http://www.zhihu.com/equation?tex=U%5Cpropto+h\" alt=\"U\\propto h\" eeimg=\"1\"&gt;）。用随机数字初始化参数等同于在某个位置给质点设定初始速度为0。这样最优化过程可以看做是模拟参数向量（即质点）在地形上滚动的过程。&lt;/p&gt;&lt;p&gt;因为作用于质点的力与梯度的潜在能量（&lt;img src=\"http://www.zhihu.com/equation?tex=F%3D-%5Cnabla+U\" alt=\"F=-\\nabla U\" eeimg=\"1\"&gt;）有关，质点&lt;b&gt;所受的力&lt;/b&gt;就是损失函数的&lt;b&gt;（负）梯度&lt;/b&gt;。还有，因为&lt;img src=\"http://www.zhihu.com/equation?tex=F%3Dma\" alt=\"F=ma\" eeimg=\"1\"&gt;，所以在这个观点下（负）梯度与质点的加速度是成比例的。注意这个理解和上面的随机梯度下降（SDG）是不同的，在普通版本中，梯度直接影响位置。而在这个版本的更新中，物理观点建议梯度只是影响速度，然后速度再影响位置：&lt;/p&gt;&lt;br&gt;&lt;div class=\"highlight\"&gt;&lt;pre&gt;&lt;code class=\"language-python\"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=\"c1\"&gt;# 动量更新&lt;/span&gt;\n&lt;span class=\"n\"&gt;v&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt; &lt;span class=\"n\"&gt;mu&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;v&lt;/span&gt; &lt;span class=\"o\"&gt;-&lt;/span&gt; &lt;span class=\"n\"&gt;learning_rate&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;dx&lt;/span&gt; &lt;span class=\"c1\"&gt;# 与速度融合&lt;/span&gt;\n&lt;span class=\"n\"&gt;x&lt;/span&gt; &lt;span class=\"o\"&gt;+=&lt;/span&gt; &lt;span class=\"n\"&gt;v&lt;/span&gt; &lt;span class=\"c1\"&gt;# 与位置融合&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在这里引入了一个初始化为0的变量&lt;b&gt;v&lt;/b&gt;和一个超参数&lt;b&gt;mu&lt;/b&gt;。说得不恰当一点，这个变量（mu）在最优化的过程中被看做&lt;i&gt;动量&lt;/i&gt;（一般值设为0.9），但其物理意义与摩擦系数更一致。这个变量有效地抑制了速度，降低了系统的动能，不然质点在山底永远不会停下来。通过交叉验证，这个参数通常设为[0.5,0.9,0.95,0.99]中的一个。和学习率随着时间退火（下文有讨论）类似，动量随时间变化的设置有时能略微改善最优化的效果，其中动量在学习过程的后阶段会上升。一个典型的设置是刚开始将动量设为0.5而在后面的多个周期（epoch）中慢慢提升到0.99。&lt;/p&gt;&lt;br&gt;&lt;blockquote&gt;&lt;p&gt;通过动量更新，参数向量会在任何有持续梯度的方向上增加速度。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;Nesterov动量&lt;/b&gt;与普通动量有些许不同，最近变得比较流行。在理论上对于凸函数它能得到更好的收敛，在实践中也确实比标准动量表现更好一些。&lt;/p&gt;&lt;br&gt;&lt;p&gt;Nesterov动量的核心思路是，当参数向量位于某个位置&lt;b&gt;x&lt;/b&gt;时，观察上面的动量更新公式可以发现，动量部分（忽视带梯度的第二个部分）会通过&lt;b&gt;mu * v&lt;/b&gt;稍微改变参数向量。因此，如果要计算梯度，那么可以将未来的近似位置&lt;b&gt;x + mu * v&lt;/b&gt;看做是“向前看”，这个点在我们一会儿要停止的位置附近。因此，计算&lt;b&gt;x + mu * v&lt;/b&gt;的梯度而不是“旧”位置&lt;b&gt;x&lt;/b&gt;的梯度就有意义了。&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;img src=\"https://pic1.zhimg.com/412afb713ddcff0ba9165ab026563304_b.png\" data-rawwidth=\"1580\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb\" width=\"1580\" data-original=\"https://pic1.zhimg.com/412afb713ddcff0ba9165ab026563304_r.png\"&gt;&lt;p&gt;Nesterov动量。既然我们知道动量将会把我们带到绿色箭头指向的点，我们就不要在原点（红色点）那里计算梯度了。使用Nesterov动量，我们就在这个“向前看”的地方计算梯度。&lt;/p&gt;&lt;br&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;也就是说，添加一些注释后，实现代码如下：&lt;br&gt;&lt;/p&gt;&lt;div class=\"highlight\"&gt;&lt;pre&gt;&lt;code class=\"language-python\"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=\"n\"&gt;x_ahead&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt; &lt;span class=\"n\"&gt;x&lt;/span&gt; &lt;span class=\"o\"&gt;+&lt;/span&gt; &lt;span class=\"n\"&gt;mu&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;v&lt;/span&gt;\n&lt;span class=\"c1\"&gt;# 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)&lt;/span&gt;\n&lt;span class=\"n\"&gt;v&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt; &lt;span class=\"n\"&gt;mu&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;v&lt;/span&gt; &lt;span class=\"o\"&gt;-&lt;/span&gt; &lt;span class=\"n\"&gt;learning_rate&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;dx_ahead&lt;/span&gt;\n&lt;span class=\"n\"&gt;x&lt;/span&gt; &lt;span class=\"o\"&gt;+=&lt;/span&gt; &lt;span class=\"n\"&gt;v&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然而在实践中，人们更喜欢和普通SGD或上面的动量方法一样简单的表达式。通过对&lt;b&gt;x_ahead = x + mu * v&lt;/b&gt;使用变量变换进行改写是可以做到的，然后用&lt;b&gt;x_ahead&lt;/b&gt;而不是&lt;b&gt;x&lt;/b&gt;来表示上面的更新。也就是说，实际存储的参数向量总是向前一步的那个版本。&lt;b&gt;x_ahead&lt;/b&gt;的公式（将其重新命名为&lt;b&gt;x&lt;/b&gt;）就变成了：&lt;/p&gt;&lt;div class=\"highlight\"&gt;&lt;pre&gt;&lt;code class=\"language-python\"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=\"n\"&gt;v_prev&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt; &lt;span class=\"n\"&gt;v&lt;/span&gt; &lt;span class=\"c1\"&gt;# 存储备份&lt;/span&gt;\n&lt;span class=\"n\"&gt;v&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt; &lt;span class=\"n\"&gt;mu&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;v&lt;/span&gt; &lt;span class=\"o\"&gt;-&lt;/span&gt; &lt;span class=\"n\"&gt;learning_rate&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;dx&lt;/span&gt; &lt;span class=\"c1\"&gt;# 速度更新保持不变&lt;/span&gt;\n&lt;span class=\"n\"&gt;x&lt;/span&gt; &lt;span class=\"o\"&gt;+=&lt;/span&gt; &lt;span class=\"o\"&gt;-&lt;/span&gt;&lt;span class=\"n\"&gt;mu&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;v_prev&lt;/span&gt; &lt;span class=\"o\"&gt;+&lt;/span&gt; &lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"mi\"&gt;1&lt;/span&gt; &lt;span class=\"o\"&gt;+&lt;/span&gt; &lt;span class=\"n\"&gt;mu&lt;/span&gt;&lt;span class=\"p\"&gt;)&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;v&lt;/span&gt; &lt;span class=\"c1\"&gt;# 位置更新变了形式&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;对于NAG（Nesterov's Accelerated Momentum）的来源和数学公式推导，我们推荐以下的拓展阅读：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Yoshua Bengio的&lt;a href=\"http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1212.0901v2.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Advances in optimizing Recurrent Networks&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;，Section 3.5。&lt;/li&gt;&lt;li&gt;&lt;a href=\"http://link.zhihu.com/?target=http%3A//www.cs.utoronto.ca/%257Eilya/pubs/ilya_sutskever_phd_thesis.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Ilya Sutskever's thesis&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt; (pdf)在section 7.2对于这个主题有更详尽的阐述。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;学习率退火&lt;/h3&gt;&lt;p&gt;在训练深度网络的时候，让学习率随着时间退火通常是有帮助的。可以这样理解：如果学习率很高，系统的动能就过大，参数向量就会无规律地跳动，不能够稳定到损失函数更深更窄的部分去。知道什么时候开始衰减学习率是有技巧的：慢慢减小它，可能在很长时间内只能是浪费计算资源地看着它混沌地跳动，实际进展很少。但如果快速地减少它，系统可能过快地失去能量，不能到达原本可以到达的最好位置。通常，实现学习率退火有3种方式：&lt;/p&gt;&lt;br&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;随步数衰减&lt;/b&gt;：每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。&lt;br&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;指数衰减&lt;/b&gt;。数学公式是&lt;img src=\"http://www.zhihu.com/equation?tex=%5Calpha%3D%5Calpha_0e%5E%7B-kt%7D\" alt=\"\\alpha=\\alpha_0e^{-kt}\" eeimg=\"1\"&gt;，其中&lt;img src=\"http://www.zhihu.com/equation?tex=%5Calpha_0%2Ck\" alt=\"\\alpha_0,k\" eeimg=\"1\"&gt;是超参数，&lt;img src=\"http://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"&gt;是迭代次数（也可以使用周期作为单位）。&lt;/li&gt;&lt;li&gt;&lt;b&gt;1/t衰减&lt;/b&gt;的数学公式是&lt;img src=\"http://www.zhihu.com/equation?tex=%5Calpha%3D%5Calpha_0%2F%281%2Bkt%29\" alt=\"\\alpha=\\alpha_0/(1+kt)\" eeimg=\"1\"&gt;，其中&lt;img src=\"http://www.zhihu.com/equation?tex=%5Calpha_0%2Ck\" alt=\"\\alpha_0,k\" eeimg=\"1\"&gt;是超参数，t是迭代次数。&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;&lt;p&gt;在实践中，我们发现随步数衰减的随机失活（dropout）更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比&lt;img src=\"http://www.zhihu.com/equation?tex=k\" alt=\"k\" eeimg=\"1\"&gt;更有解释性。最后，如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。&lt;/p&gt;&lt;h3&gt;二阶方法&lt;/h3&gt;&lt;p&gt;在深度网络背景下，第二类常用的最优化方法是基于&lt;a href=\"http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Newton%2527s_method_in_optimization\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;牛顿法&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;的，其迭代如下：&lt;br&gt;&lt;/p&gt;&lt;img src=\"http://www.zhihu.com/equation?tex=%5Cdisplaystyle+x%5Cleftarrow+x-%5BHf%28x%29%5D%5E%7B-1%7D%5Cnabla+f%28x%29\" alt=\"\\displaystyle x\\leftarrow x-[Hf(x)]^{-1}\\nabla f(x)\" eeimg=\"1\"&gt;&lt;br&gt;&lt;p&gt;这里&lt;img src=\"http://www.zhihu.com/equation?tex=Hf%28x%29\" alt=\"Hf(x)\" eeimg=\"1\"&gt;是&lt;a href=\"http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Hessian_matrix\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Hessian矩阵&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;，它是函数的二阶偏导数的平方矩阵。&lt;img src=\"http://www.zhihu.com/equation?tex=%5Cnabla+f%28x%29\" alt=\"\\nabla f(x)\" eeimg=\"1\"&gt;是梯度向量，这和梯度下降中一样。直观理解上，Hessian矩阵描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。需要重点注意的是，在这个公式中是没有学习率这个超参数的，这相较于一阶方法是一个巨大的优势。&lt;/p&gt;&lt;p&gt;然而上述更新方法很难运用到实际的深度学习应用中去，这是因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间。举例来说，假设一个有一百万个参数的神经网络，其Hessian矩阵大小就是[1,000,000 x 1,000,000]，将占用将近3,725GB的内存。这样，各种各样的&lt;i&gt;拟&lt;/i&gt;-牛顿法就被发明出来用于近似转置Hessian矩阵。在这些方法中最流行的是&lt;a href=\"http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Limited-memory_BFGS\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;L-BFGS&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;，该方法使用随时间的梯度中的信息来隐式地近似（也就是说整个矩阵是从来没有被计算的）。&lt;/p&gt;&lt;p&gt;然而，即使解决了存储空间的问题，L-BFGS应用的一个巨大劣势是需要对整个训练集进行计算，而整个训练集一般包含几百万的样本。和小批量随机梯度下降（mini-batch SGD）不同，让L-BFGS在小批量上运行起来是很需要技巧，同时也是研究热点。&lt;/p&gt;&lt;p&gt;&lt;b&gt;实践&lt;/b&gt;。在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。&lt;/p&gt;&lt;p&gt;参考资料：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href=\"http://link.zhihu.com/?target=http%3A//research.google.com/archive/large_deep_networks_nips2012.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Large Scale Distributed Deep Networks&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt; 一文来自谷歌大脑团队，比较了在大规模数据情况下L-BFGS和SGD算法的表现。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;a href=\"http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1311.2115\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;SFO&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;算法想要把SGD和L-BFGS的优势结合起来。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;逐参数适应学习率方法&lt;/h2&gt;&lt;p&gt;前面讨论的所有方法都是对学习率进行全局地操作，并且对所有的参数都是一样的。学习率调参是很耗费计算资源的过程，所以很多工作投入到发明能够适应性地对学习率调参的方法，甚至是逐个参数适应学习率调参。很多这些方法依然需要其他的超参数设置，但是其观点是这些方法对于更广范围的超参数比原始的学习率方法有更良好的表现。在本小节我们会介绍一些在实践中可能会遇到的常用适应算法：&lt;/p&gt;&lt;p&gt;&lt;b&gt;Adagrad&lt;/b&gt;是一个由&lt;a href=\"http://link.zhihu.com/?target=http%3A//jmlr.org/papers/v12/duchi11a.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Duchi等&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;提出的适应性学习率算法&lt;/p&gt;&lt;div class=\"highlight\"&gt;&lt;pre&gt;&lt;code class=\"language-python\"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=\"c1\"&gt;# 假设有梯度和参数向量x&lt;/span&gt;\n&lt;span class=\"n\"&gt;cache&lt;/span&gt; &lt;span class=\"o\"&gt;+=&lt;/span&gt; &lt;span class=\"n\"&gt;dx&lt;/span&gt;&lt;span class=\"o\"&gt;**&lt;/span&gt;&lt;span class=\"mi\"&gt;2&lt;/span&gt;\n&lt;span class=\"n\"&gt;x&lt;/span&gt; &lt;span class=\"o\"&gt;+=&lt;/span&gt; &lt;span class=\"o\"&gt;-&lt;/span&gt; &lt;span class=\"n\"&gt;learning_rate&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;dx&lt;/span&gt; &lt;span class=\"o\"&gt;/&lt;/span&gt; &lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"n\"&gt;np&lt;/span&gt;&lt;span class=\"o\"&gt;.&lt;/span&gt;&lt;span class=\"n\"&gt;sqrt&lt;/span&gt;&lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"n\"&gt;cache&lt;/span&gt;&lt;span class=\"p\"&gt;)&lt;/span&gt; &lt;span class=\"o\"&gt;+&lt;/span&gt; &lt;span class=\"n\"&gt;eps&lt;/span&gt;&lt;span class=\"p\"&gt;)&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意，变量&lt;b&gt;cache&lt;/b&gt;的尺寸和梯度矩阵的尺寸是一样的，还跟踪了每个参数的梯度的平方和。这个一会儿将用来归一化参数更新步长，归一化是逐元素进行的。注意，接收到高梯度值的权重更新的效果被减弱，而接收到低梯度值的权重的更新效果将会增强。有趣的是平方根的操作非常重要，如果去掉，算法的表现将会糟糕很多。用于平滑的式子&lt;b&gt;eps&lt;/b&gt;（一般设为1e-4到1e-8之间）是防止出现除以0的情况。Adagrad的一个缺点是，在深度学习中单调的学习率被证明通常过于激进且过早停止学习。&lt;/p&gt;&lt;p&gt;&lt;b&gt;RMSprop&lt;/b&gt;。是一个非常高效，但没有公开发表的适应性学习率方法。有趣的是，每个使用这个方法的人在他们的论文中都引用自Geoff Hinton的Coursera课程的&lt;a href=\"http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%257Etijmen/csc321/slides/lecture_slides_lec6.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;第六课的第29页PPT&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;。这个方法用一种很简单的方式修改了Adagrad方法，让它不那么激进，单调地降低了学习率。具体说来，就是它使用了一个梯度平方的滑动平均：&lt;/p&gt;&lt;div class=\"highlight\"&gt;&lt;pre&gt;&lt;code class=\"language-python\"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=\"n\"&gt;cache&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt;  &lt;span class=\"n\"&gt;decay_rate&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;cache&lt;/span&gt; &lt;span class=\"o\"&gt;+&lt;/span&gt; &lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"mi\"&gt;1&lt;/span&gt; &lt;span class=\"o\"&gt;-&lt;/span&gt; &lt;span class=\"n\"&gt;decay_rate&lt;/span&gt;&lt;span class=\"p\"&gt;)&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;dx&lt;/span&gt;&lt;span class=\"o\"&gt;**&lt;/span&gt;&lt;span class=\"mi\"&gt;2&lt;/span&gt;\n&lt;span class=\"n\"&gt;x&lt;/span&gt; &lt;span class=\"o\"&gt;+=&lt;/span&gt; &lt;span class=\"o\"&gt;-&lt;/span&gt; &lt;span class=\"n\"&gt;learning_rate&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;dx&lt;/span&gt; &lt;span class=\"o\"&gt;/&lt;/span&gt; &lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"n\"&gt;np&lt;/span&gt;&lt;span class=\"o\"&gt;.&lt;/span&gt;&lt;span class=\"n\"&gt;sqrt&lt;/span&gt;&lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"n\"&gt;cache&lt;/span&gt;&lt;span class=\"p\"&gt;)&lt;/span&gt; &lt;span class=\"o\"&gt;+&lt;/span&gt; &lt;span class=\"n\"&gt;eps&lt;/span&gt;&lt;span class=\"p\"&gt;)&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在上面的代码中，decay_rate是一个超参数，常用的值是[0.9,0.99,0.999]。其中&lt;b&gt;x+=&lt;/b&gt;和Adagrad中是一样的，但是&lt;b&gt;cache&lt;/b&gt;变量是不同的。因此，RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，这同样效果不错。但是和Adagrad不同，其更新不会让学习率单调变小。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Adam&lt;/b&gt;。&lt;a href=\"http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1412.6980\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Adam&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;是最近才提出的一种更新方法，它看起来像是RMSProp的动量版。简化的代码是下面这样：&lt;/p&gt;&lt;div class=\"highlight\"&gt;&lt;pre&gt;&lt;code class=\"language-python\"&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=\"n\"&gt;m&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt; &lt;span class=\"n\"&gt;beta1&lt;/span&gt;&lt;span class=\"o\"&gt;*&lt;/span&gt;&lt;span class=\"n\"&gt;m&lt;/span&gt; &lt;span class=\"o\"&gt;+&lt;/span&gt; &lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"mi\"&gt;1&lt;/span&gt;&lt;span class=\"o\"&gt;-&lt;/span&gt;&lt;span class=\"n\"&gt;beta1&lt;/span&gt;&lt;span class=\"p\"&gt;)&lt;/span&gt;&lt;span class=\"o\"&gt;*&lt;/span&gt;&lt;span class=\"n\"&gt;dx&lt;/span&gt;\n&lt;span class=\"n\"&gt;v&lt;/span&gt; &lt;span class=\"o\"&gt;=&lt;/span&gt; &lt;span class=\"n\"&gt;beta2&lt;/span&gt;&lt;span class=\"o\"&gt;*&lt;/span&gt;&lt;span class=\"n\"&gt;v&lt;/span&gt; &lt;span class=\"o\"&gt;+&lt;/span&gt; &lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"mi\"&gt;1&lt;/span&gt;&lt;span class=\"o\"&gt;-&lt;/span&gt;&lt;span class=\"n\"&gt;beta2&lt;/span&gt;&lt;span class=\"p\"&gt;)&lt;/span&gt;&lt;span class=\"o\"&gt;*&lt;/span&gt;&lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"n\"&gt;dx&lt;/span&gt;&lt;span class=\"o\"&gt;**&lt;/span&gt;&lt;span class=\"mi\"&gt;2&lt;/span&gt;&lt;span class=\"p\"&gt;)&lt;/span&gt;\n&lt;span class=\"n\"&gt;x&lt;/span&gt; &lt;span class=\"o\"&gt;+=&lt;/span&gt; &lt;span class=\"o\"&gt;-&lt;/span&gt; &lt;span class=\"n\"&gt;learning_rate&lt;/span&gt; &lt;span class=\"o\"&gt;*&lt;/span&gt; &lt;span class=\"n\"&gt;m&lt;/span&gt; &lt;span class=\"o\"&gt;/&lt;/span&gt; &lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"n\"&gt;np&lt;/span&gt;&lt;span class=\"o\"&gt;.&lt;/span&gt;&lt;span class=\"n\"&gt;sqrt&lt;/span&gt;&lt;span class=\"p\"&gt;(&lt;/span&gt;&lt;span class=\"n\"&gt;v&lt;/span&gt;&lt;span class=\"p\"&gt;)&lt;/span&gt; &lt;span class=\"o\"&gt;+&lt;/span&gt; &lt;span class=\"n\"&gt;eps&lt;/span&gt;&lt;span class=\"p\"&gt;)&lt;/span&gt;\n&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意这个更新方法看起来真的和RMSProp很像，除了使用的是平滑版的梯度&lt;b&gt;m&lt;/b&gt;，而不是用的原始梯度向量&lt;b&gt;dx&lt;/b&gt;。论文中推荐的参数值&lt;b&gt;eps=1e-8, beta1=0.9, beta2=0.999&lt;/b&gt;。在实际操作中，我们推荐Adam作为默认的算法，一般而言跑起来比RMSProp要好一点。但是也可以试试SGD+Nesterov动量。完整的Adam更新算法也包含了一个偏置&lt;em&gt;（bias）矫正&lt;/em&gt;机制，因为&lt;b&gt;m,v&lt;/b&gt;两个矩阵初始为0，在没有完全热身之前存在偏差，需要采取一些补偿措施。建议读者可以阅读论文查看细节，或者课程的PPT。&lt;/p&gt;&lt;p&gt;拓展阅读：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=\"http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1312.6055\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Unit Tests for Stochastic Optimization&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;一文展示了对于随机最优化的测试。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img src=\"https://pic4.zhimg.com/7fd7404711c99456237cdff7b3a3bad7_b.png\" data-rawwidth=\"1598\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb\" width=\"1598\" data-original=\"https://pic4.zhimg.com/7fd7404711c99456237cdff7b3a3bad7_r.png\"&gt;&lt;b&gt;&lt;i&gt;译者注：上图原文中为动图，知乎专栏不支持动图，知友可点击&lt;a href=\"http://link.zhihu.com/?target=http%3A//cs231n.github.io/neural-networks-3/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;原文链接&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;查看。&lt;/i&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;上面的动画可以帮助你理解学习的动态过程。&lt;b&gt;左边&lt;/b&gt;是一个损失函数的等高线图，上面跑的是不同的最优化算法。注意基于动量的方法出现了射偏了的情况，使得最优化过程看起来像是一个球滚下山的样子。&lt;b&gt;右边&lt;/b&gt;展示了一个马鞍状的最优化地形，其中对于不同维度它的曲率不同（一个维度下降另一个维度上升）。注意SGD很难突破对称性，一直卡在顶部。而RMSProp之类的方法能够看到马鞍方向有很低的梯度。因为在RMSProp更新方法中的分母项，算法提高了在该方向的有效学习率，使得RMSProp能够继续前进。图片版权：&lt;a href=\"http://link.zhihu.com/?target=https%3A//twitter.com/alecrad\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Alec Radford&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;。&lt;/p&gt;&lt;br&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;h2&gt;超参数调优&lt;/h2&gt;&lt;p&gt;我们已经看到，训练一个神经网络会遇到很多超参数设置。神经网络最常用的设置有：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;初始学习率。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;学习率衰减方式（例如一个衰减常量）。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;正则化强度（L2惩罚，随机失活强度）。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;但是也可以看到，还有很多相对不那么敏感的超参数。比如在逐参数适应学习方法中，对于动量及其时间表的设置等。在本节中将介绍一些额外的调参要点和技巧：&lt;/p&gt;&lt;p&gt;&lt;b&gt;实现&lt;/b&gt;。更大的神经网络需要更长的时间去训练，所以调参可能需要几天甚至几周。记住这一点很重要，因为这会影响你设计代码的思路。一个具体的设计是用&lt;b&gt;仆程序&lt;/b&gt;持续地随机设置参数然后进行最优化。在训练过程中，&lt;b&gt;仆程序&lt;/b&gt;会对每个周期后验证集的准确率进行监控，然后向文件系统写下一个模型的记录点（记录点中有各种各样的训练统计数据，比如随着时间的损失值变化等），这个文件系统最好是可共享的。在文件名中最好包含验证集的算法表现，这样就能方便地查找和排序了。然后还有一个&lt;b&gt;主程序&lt;/b&gt;，它可以启动或者结束计算集群中的&lt;b&gt;仆程序&lt;/b&gt;，有时候也可能根据条件查看&lt;b&gt;仆程序&lt;/b&gt;写下的记录点，输出它们的训练统计数据等。&lt;/p&gt;&lt;p&gt;&lt;b&gt;比起交叉验证最好使用一个验证集&lt;/b&gt;。在大多数情况下，一个尺寸合理的验证集可以让代码更简单，不需要用几个数据集来交叉验证。你可能会听到人们说他们“交叉验证”一个参数，但是大多数情况下，他们实际是使用的一个验证集。&lt;/p&gt;&lt;p&gt;&lt;b&gt;超参数范围&lt;/b&gt;。在对数尺度上进行超参数搜索。例如，一个典型的学习率应该看起来是这样：&lt;b&gt;learning_rate = 10 ** uniform(-6, 1)&lt;/b&gt;。也就是说，我们从标准分布中随机生成了一个数字，然后让它成为10的阶数。对于正则化强度，可以采用同样的策略。直观地说，这是因为学习率和正则化强度都对于训练的动态进程有乘的效果。例如：当学习率是0.001的时候，如果对其固定地增加0.01，那么对于学习进程会有很大影响。然而当学习率是10的时候，影响就微乎其微了。这就是因为学习率乘以了计算出的梯度。因此，比起加上或者减少某些值，思考学习率的范围是乘以或者除以某些值更加自然。但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：&lt;b&gt;dropout=uniform(0,1)&lt;/b&gt;）。&lt;/p&gt;&lt;p&gt;&lt;b&gt;随机搜索优于网格搜索&lt;/b&gt;。Bergstra和Bengio在文章&lt;a href=\"http://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Random Search for Hyper-Parameter Optimization&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;中说“随机选择比网格化的选择更加有效”，而且在实践中也更容易实现。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img src=\"https://pic4.zhimg.com/d25cf561835c7b96ae6d1c91868bcbff_b.png\" data-rawwidth=\"1596\" data-rawheight=\"432\" class=\"origin_image zh-lightbox-thumb\" width=\"1596\" data-original=\"https://pic4.zhimg.com/d25cf561835c7b96ae6d1c91868bcbff_r.png\"&gt;在&lt;a href=\"http://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Random Search for Hyper-Parameter Optimization&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;中的核心说明图。通常，有些超参数比其余的更重要，通过随机搜索，而不是网格化的搜索，可以让你更精确地发现那些比较重要的超参数的好数值。&lt;br&gt;&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;b&gt;对于边界上的最优值要小心&lt;/b&gt;。这种情况一般发生在你在一个不好的范围内搜索超参数（比如学习率）的时候。比如，假设我们使用&lt;b&gt;learning_rate = 10 ** uniform(-6,1)&lt;/b&gt;来进行搜索。一旦我们得到一个比较好的值，一定要确认你的值不是出于这个范围的边界上，不然你可能错过更好的其他搜索范围。&lt;/p&gt;&lt;p&gt;&lt;b&gt;从粗到细地分阶段搜索&lt;/b&gt;。在实践中，先进行初略范围（比如10 ** [-6, 1]）搜索，然后根据好的结果出现的地方，缩小范围进行搜索。进行粗搜索的时候，让模型训练一个周期就可以了，因为很多超参数的设定会让模型没法学习，或者突然就爆出很大的损失值。第二个阶段就是对一个更小的范围进行搜索，这时可以让模型运行5个周期，而最后一个阶段就在最终的范围内进行仔细搜索，运行很多次周期。&lt;/p&gt;&lt;p&gt;&lt;b&gt;贝叶斯超参数最优化&lt;/b&gt;是一整个研究领域，主要是研究在超参数空间中更高效的导航算法。其核心的思路是在不同超参数设置下查看算法性能时，要在探索和使用中进行合理的权衡。基于这些模型，发展出很多的库，比较有名的有： &lt;a href=\"http://link.zhihu.com/?target=https%3A//github.com/JasperSnoek/spearmint\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Spearmint&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;, &lt;a href=\"http://link.zhihu.com/?target=http%3A//www.cs.ubc.ca/labs/beta/Projects/SMAC/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;SMAC&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;, 和&lt;a href=\"http://link.zhihu.com/?target=http%3A//jaberg.github.io/hyperopt/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Hyperopt&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;。然而，在卷积神经网络的实际使用中，比起上面介绍的先认真挑选的一个范围，然后在该范围内随机搜索的方法，这个方法还是差一些。&lt;a href=\"http://link.zhihu.com/?target=http%3A//nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;这里&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;有更详细的讨论。&lt;/p&gt;&lt;h2&gt;评价&lt;/h2&gt;&lt;h3&gt;模型集成&lt;/h3&gt;&lt;p&gt;在实践的时候，有一个总是能提升神经网络几个百分点准确率的办法，就是在训练的时候训练几个独立的模型，然后在测试的时候平均它们预测结果。集成的模型数量增加，算法的结果也单调提升（但提升效果越来越少）。还有模型之间的差异度越大，提升效果可能越好。进行集成有以下几种方法：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;同一个模型，不同的初始化&lt;/b&gt;。使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。这种方法的风险在于多样性只来自于不同的初始化条件。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;在交叉验证中发现最好的模型&lt;/b&gt;。使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。这样就提高了集成的多样性，但风险在于可能会包含不够理想的模型。在实际操作中，这样操作起来比较简单，在交叉验证后就不需要额外的训练了。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;一个模型设置多个记录点&lt;/b&gt;。如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。很显然，这样做多样性不足，但是在实践中效果还是不错的，这种方法的优势是代价比较小。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;在训练的时候跑参数的平均值&lt;/b&gt;。和上面一点相关的，还有一个也能得到1-2个百分点的提升的小代价方法，这个方法就是在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。这样你就对前几次循环中的网络状态进行了平均。你会发现这个“平滑”过的版本的权重总是能得到更少的误差。直观的理解就是目标函数是一个碗状的，你的网络在这个周围跳跃，所以对它们平均一下，就更可能跳到中心去。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;模型集成的一个劣势就是在测试数据的时候会花费更多时间。最近Geoff Hinton在“&lt;a href=\"http://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DEK61htlw8hY\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Dark Knowledge&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;”上的工作很有启发：其思路是通过将集成似然估计纳入到修改的目标函数中，从一个好的集成中抽出一个单独模型。&lt;/p&gt;&lt;h2&gt;总结&lt;/h2&gt;&lt;p&gt;训练一个神经网络需要：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;利用小批量数据对实现进行梯度检查，还要注意各种错误。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;进行合理性检查，确认初始损失值是合理的，在小数据集上能得到100%的准确率。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;在训练时，跟踪损失函数值，训练集和验证集准确率，如果愿意，还可以跟踪更新的参数量相对于总参数量的比例（一般在1e-3左右），然后如果是对于卷积神经网络，可以将第一层的权重可视化。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;推荐的两个更新方法是SGD+Nesterov动量方法，或者Adam方法。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;随着训练进行学习率衰减。比如，在固定多少个周期后让学习率减半，或者当验证集准确率下降的时候。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;使用随机搜索（不要用网格搜索）来搜索最优的超参数。分阶段从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地来搜索。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;进行模型集成来获得额外的性能提高。&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;拓展阅读&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Leon Bottou的《&lt;a href=\"http://link.zhihu.com/?target=http%3A//research.microsoft.com/pubs/192769/tricks-2012.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;SGD要点和技巧&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;》。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Yann LeCun的《&lt;a href=\"http://link.zhihu.com/?target=http%3A//yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Efficient BackProp&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;》。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;Yoshua Bengio的《&lt;a href=\"http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1206.5533v2.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Practical Recommendations for Gradient-Based Training of Deep Architectures&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;》。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;译者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;转载须全文转载且注明原文链接&lt;/b&gt;，否则保留维权权利；&lt;/li&gt;&lt;li&gt;请知友们通过评论和私信等方式批评指正，贡献者均会补充提及。&lt;/li&gt;&lt;/ol&gt;","state":"published","sourceUrl":"","pageCommentsCount":0,"canComment":true,"snapshotUrl":"","slug":21798784,"publishedTime":"2016-08-09T15:12:17+08:00","url":"/p/21798784","title":"CS231n课程笔记翻译：神经网络笔记3（下）","summary":"译者注：本文&lt;a href=\"https://zhuanlan.zhihu.com/intelligentunit\" class=\"internal\"&gt;智能单元&lt;/a&gt;首发，译自斯坦福CS231n课程笔记&lt;a href=\"http://link.zhihu.com/?target=http%3A//cs231n.github.io/neural-networks-3/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Neural Nets notes 3&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;，课程教师&lt;a href=\"http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"&gt;Andrej Karpathy&lt;i class=\"icon-external\"&gt;&lt;/i&gt;&lt;/a&gt;授权翻译。本篇教程由&lt;a href=\"https://www.zhihu.com/people/du-ke\" class=\"internal\"&gt;杜客&lt;/a&gt;翻译完成，&lt;a href=\"https://www.zhihu.com/people/kun-kun-97-81\" class=\"internal\"&gt;堃堃&lt;/a&gt;和&lt;a href=\"https://www.zhihu.com/people/hmonkey\" class=\"internal\"&gt;巩子嘉&lt;/a&gt;进行校对修改。译文含公式和代码，建议PC端阅读。原文如下内容列表：梯度检查合理性（Sanity）检查检查学…","reviewingCommentsCount":0,"meta":{"previous":null,"next":null},"commentPermission":"anyone","commentsCount":22,"likesCount":91}},"annotationDetail":null,"commentsCount":17,"likesCount":128,"FULLINFO":true}},"User":{"xhu2013":{"isFollowed":false,"name":"xhu2013","headline":"","avatarUrl":"https://pic1.zhimg.com/e78eac09c_s.jpg","isFollowing":false,"type":"people","slug":"xhu2013","bio":"多看，多听，多想，少说话","hash":"a6468e23d117bd6ad93acb92a93141d5","uid":33283320578048,"links":{"columns":"/api/me/columns"},"isOrg":false,"pendingColumns":[],"activated":true,"allowShareDaily":false,"isBindPhone":false,"mutedInfo":{"muted":false,"reason":null},"description":"","muted":false,"profileUrl":"https://www.zhihu.com/people/xhu2013","avatar":{"id":"e78eac09c","template":"https://pic1.zhimg.com/{id}_{size}.jpg"},"isOrgWhiteList":false,"email":"chenyong679@msn.cn","columns":[]},"flood-sung":{"isFollowed":false,"name":"Flood Sung","headline":"本人已委托“维权骑士”（rightknights.com)为我的文章进行维权行动，如需转载前往https://rightknights.com/material/author?id=2960 获取合法授权。联系本人有问题直接问，除了技术问题之外，其余一律不答！谢谢！","avatarUrl":"https://pic3.zhimg.com/73a71f47d66e280735a6c786131bdfe2_s.jpg","isFollowing":false,"type":"people","slug":"flood-sung","bio":"Independent Researcher of Deep Learning","hash":"23deec836a24f295500a6d740011359c","uid":654375804428095500,"isOrg":false,"description":"本人已委托“维权骑士”（rightknights.com)为我的文章进行维权行动，如需转载前往https://rightknights.com/material/author?id=2960 获取合法授权。联系本人有问题直接问，除了技术问题之外，其余一律不答！谢谢！","profileUrl":"https://www.zhihu.com/people/flood-sung","avatar":{"id":"73a71f47d66e280735a6c786131bdfe2","template":"https://pic3.zhimg.com/{id}_{size}.jpg"},"isOrgWhiteList":false,"badge":{"identity":null,"bestAnswerer":null}}},"Comment":{},"favlists":{}},"me":{"slug":"xhu2013"},"global":{},"columns":{"intelligentunit":{"following":false,"canManage":false,"href":"/api/columns/intelligentunit","name":"智能单元","creator":{"slug":"du-ke"},"url":"/intelligentunit","slug":"intelligentunit","avatar":{"id":"4a97d93d652f45ededf2ebab9a13f22b","template":"https://pic4.zhimg.com/{id}_{size}.jpeg"}}},"columnPosts":{},"columnSettings":{"colomnAuthor":[],"uploadAvatarDetails":"","contributeRequests":[],"contributeRequestsTotalCount":0,"inviteAuthor":""},"postComments":{},"postReviewComments":{"comments":[],"newComments":[],"hasMore":true},"favlistsByUser":{},"favlistRelations":{},"promotions":{},"switches":{"couldAddVideo":false},"draft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null}},"drafts":{"draftsList":[],"next":{}},"config":{"userNotBindPhoneTipString":{}},"recommendPosts":{"articleRecommendations":[],"columnRecommendations":[]},"env":{"isAppView":false,"appViewConfig":{"content_padding_top":128,"content_padding_bottom":56,"content_padding_left":16,"content_padding_right":16,"title_font_size":22,"body_font_size":16,"is_dark_theme":false,"can_auto_load_image":true,"app_info":"OS=iOS"},"isApp":false},"sys":{}}</textarea>

    
    <script src="./智能单元 - 知乎专栏_files/common.fea1566d75e4b79d9452.js.下载"></script>
<script src="./智能单元 - 知乎专栏_files/app.65d5d95c881c2d404ddd.js.下载"></script>
<script src="./智能单元 - 知乎专栏_files/raven.043900884c212f4c184c.js.下载" async="" defer=""></script>
  

<div><div data-reactroot=""><div></div></div></div></body></html>